{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WulaLbwdw5Ex",
        "outputId": "e5f11a20-47a7-4ab7-ea3d-4a4d97220aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m858.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.3.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.0.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.41.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.66.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->bert-score)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2024.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert-score\n",
            "Successfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install bert-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluations = ['''6 Results We examine the model’s response against 6 levels of encoding of the input sequence. An ascent in Rouge Scores is observed until 3 encoding levels while a fall-off appears from the 4th level on-wards. The experiment is terminated at the 6th level considering the increase in declivity of the model’s performance as displayed in Fig. (3a and b). 6.1 Computational costs On the Gigaword dataset, MED’s training time per epoch is round about 25.64 hours for 1-Level encoding model, 26.77 hours for 2-Level encoding model, 27.52 hours for 3-Level encoding model, 28.65 hours for 4-Level encoding model, 29.78 hours for 5-Level encoding model, and 30.54 hours for 6-Level encoding model. We trained each one of our models for 3 epochs, on a 12GB Nvidia Geforce GTX 1080ti GPU. Our finest model, MED (beam ? 3-Level) took about 82.56 hours in total to train. All models took about 28.15 hours per epoch on average and typically converged within 3 epochs. The wall-clock training time until convergence varies between 3 to 4 days depending on the encoding levels as shown in Table (8). The observations suggest that with successive increase in encoding levels the model takes on an average around 1 hour/epoch longer to train. This stunted rise in training time is likely to be a consequence of weight-sharing across different encoding levels which reuses the same weights, that is, it does not increase the number of trainable parameters with increase in encoding levels. 6.2 Rouge results on Gigaword Test Corpus - 1951 The Rouge Scores conforming with six shared-weight encoding levels of the input sequence are reported in Table (4 5 and 6). Against two levels, the model performs moderately better yielding about 5% increase in R1, 11% increase in R2, and 7% increase in RL Scores w.r.t the F1 variant. Against three levels, the model administers conspicuously better results with about 12% increase in R1, 21% increase in R2, and 13% increase in RL Scores w.r.t the F1 variant, as illustrated in Table (7). The model encounters a sharp downturn in Rouge 1, 2, and L scores for four, five, and six levels, where the accuracy falls even below that obtained for a single encoding. Figure (3a) showcases the upgrade and downgrade in Rouge F1 Scores adhering to six encoding levels on the Gigaword Test Set1951. The sink in accuracy could be a consequence of over-learning/over-fitting or vanishing gradients due to the architecture’s high-dimensionality (Table 8). Our model’s performance compared with the baselines mentioned earlier on the Gigaword corpus is presented in Table (2). MED after three levels of encoding produces a Rouge-1 F1 Score of 36.72 and performs significantly better than the ABS, ABS?, RAS-LSTM, RAS-Elman, Struct2Way, SEASS, DRGD, Featseq2-seq, and ten other baseline models. Our model generates a Rouge-2 F1 Score of 17.61 and performs significantly better than the ABS, ABS?, RAS-LSTM, RAS-Elman, LuongNMT, OpenNMT, SelectiveMTL, s2s?e2t, and eight other baseline models. MED outperforms all the baseline models in terms of Rouge-L F1 Score with the highest score of 35.39 after undergoing three levels of encoding of the input sequence. 6.3 Rouge results on DUC2004 Task1 Corpus The Rouge Scores (R1, R2, and RL) on DUC2004 Task1 corpora for six levels of encoding are conveyed in Table [9]. A consistent trend is observed with increase in the number of encoding levels from 1 to 3 (upslope) and 4 to 6 (downslope) showcased in Fig. (3b). Our model’s performance compared with the baseline mentioned above on the DUC 2004 Task1 corpus is communicated in Table (2). The MED model after three levels of sharedweight encoding of the input sequence produces an R1 F1 Score of 29.73, R2 F1 Score of 9.64, and RL F1 Score of 26.11, surpassing the scores of ABS, ABS?, RAS-LSTM, RAS-Elman, LenEmb, TOPIARY, MOSES?, Featseq2seq, SelectiveMTL, SEASS, and GLEAM w.r.t R1 and RL. 6.4 Rouge results on internal Gigaword Test Corpus - 10,000 The Rouge Scores (Recall, Precision, and F1) on the internal Gigaword test set consisting of 10,000 samples is presented in Table (6). It shows that the MED, upon three levels of shared-weight encoding of the input sequence, generates a Rouge-1 Recall of 46.02 and precision of 46.12, which indicates that there are 46% matching unigrams in the predicted and actual summaries. Rouge-L Recall of 44.78 is obtained, which is a measure of the model’s ability to capture the information content and grammaticality. 6.5 Human evaluation To substantiate claims of enhanced readability and grammaticality using multiple levels of encoding of the input sequence, we implored a group of eight human-annotators for assessing the quality of our modeled abstracts. Our annotators consist of three senior researchers and five interdisciplinary professionals belonging to medi-cine, management, and teaching. We gauge the summaries w.r.t four parameters, namely, 1) Grammaticality: Degree to which summaries obey the rules of English grammar, 2) Informativeness: Degree to which summaries cover essential aspects of given literature, 3) Veracity: Degree to which summaries preserve the original meaning of source literature, 4) Readability: How effortlessly the underlying idea is expressed in the summary. We sampled 100 instances from the Gigaword test-set for 1, 2, and 3 encoding levels, respectively, and requested the annotators to rate each summary using a Likert scale, ranging from 1 (very poor) - 5 (very good). Conforming to the results reported in Table (10), our model rendered summaries having high-linguistic quality (in terms of grammaticality and readability) with 2 and 3 levels of encoding as compared to a single one. Figure (4a) displays a sizable increase in grammaticality and readability scores while a moderate rise in informativeness and veracity scores for 1, 2, and 3 encoding levels, respectively. Inter-rater Reliability: In order to substantiate the reliability of our annotators, we rendered Cohen’s Kappa [49, 50] reliability statistic to gauge the degree to which our annotators (raters) assigned the same rating to the same parameter. The Cohen’s kappa j is computed as: j ¼ pðaÞ pðeÞ 1 pðeÞ ð20Þ where p(a) indicates the actual observed agreement, and p(e) indicates chance agreement. For the MED(beam ? 3-Level) model, we computed the average kappa between every of annotators, by combining all four categories. As illustrated in Fig. (4b), the obtained kappa values range between 0.758 and 0.807 .These estimates signify moderate to strong agreements between the raters. Finally, the average kappa for MED(beam ? 3-Level) model is 0.782 indicating substantial agreement between the annotators.''', '''Results Patients The total data set included 1,822 patients with unilateral TSFs that were treated operatively. The median age of the patients was 38 years (range, 14 to 92 years); 75% of the patients were men. Forty-five percent of the patients had a closed fracture. Other patient and fracture characteristics are displayed in Table II. A total of 170 patients (9%) developed an infection that required treatment: 62 patients (3%) were treated nonoperatively with intravenous antibiotics and 108 patients (6%), in addition to being administered antibiotics, were treated operatively. Feature Selection Feature selection in the derivation cohort using random forest algorithms identified 7 variables that were relevant for algorithm development. In order of importance, these variables were (1) Gustilo-Anderson classification or Tscherne classification, (2) bone loss, (3) mechanism of injury, (4) multitrauma, (5) AO/OTA classification, (6) age, and (7) location (Fig. 1). Performance of ML Prediction Models in the Derivation Cohort Discriminative performance of the 5 algorithms as quantified by the AUC ranged from 0.67 to 0.75 (Table III) (see Appendix 1). Fig. 1 Variable importance based on feature selection using random forest algorithms. TABLE III Performance of ML Algorithms in Predicting Subsequent Surgery in the Derivation Cohort After 10-Fold Cross-Validation Was Repeated 3 Times* Model AUC Calibration Slope Calibration Intercept Brier Score† Bayes point machine 0.74 (0.71 to 0.77) 0.84 (0.74 to 0.94) 20.06 (20.17 to 0.05) 0.077 Boosted decision tree 0.74 (0.71 to 0.76) 0.83 (0.74 to 0.93) 20.00 (20.11 to 0.10) 0.077 Penalized logistic regression 0.75 (0.72 to 0.77) 0.94 (0.83 to 1.04) 0.00 (20.10 to 0.11) 0.076 Neural network 0.73 (0.70 to 0.76) 0.69 (0.60 to 0.78) 20.14 (20.26 to 20.03) 0.079 Support vector machine 0.67 (0.64 to 0.70) 0.76 (20.64 to 0.88) 20.00 (20.11 to 0.10) 0.080 *N = 1,458. The values are given as the AUC, with the 95% confidence interval in parentheses. †Upper limit of the Brier score = 0.085. 535 TH E JOURNAL OF BONE & JOINT SURGERY d JBJS . ORG VOLUME 103-A d NUMBER 6 d MARCH 17, 2021 MACHINE LEARNING ALGORITHM FOR TIBIAL SHAFT FRACTURES AT RISK FOR INFECTION AFTER SURGERY Calibration slopes ranged from 0.69 to 0.94. Calibration intercepts ranged from –0.14 to 0.00 (see Appendix 2). The Brier score ranged from 0.076 to 0.080. The upper limit of the Brier score was 0.085, based on an incidence of infection of 9.3%. Based on the numeric assessment of the 4 performance measures as well as on the graphical assessment of the calibration curves, the penalized logistic regression, Bayes point machine, and boosted decision tree-derived models outperformed the neural network and support vector machine models. The predictive performance of these 3 ML prediction models was further evaluated in the validation cohort. Performance of Best-Performing ML Models in the Validation Cohort Discriminative performance of the 3 algorithms in the validation cohort as quantified by the AUC ranged from 0.80 to 0.82 (Table IV) (see Appendix 3). The calibration slopes ranged from 0.83 to 1.07. The calibration intercepts ranged from 0.04 to 0.11 (see Appendix 4). The Brier score ranged from 0.078 to 0.083, relative to the upper limit of 0.085. The Bayes point machine and penalized logistic regression models showed similar performance, outperforming the boosted decision tree model. Final Model Based on better calibration in the derivation cohort (slope, 0.94 versus 0.84; intercept, 0.00 versus 20.06) as well as similar performance, the penalized logistic regression-derived prediction model was deemed superior to the Bayes point machine model and was therefore chosen as the final model. GustiloAnderson type IIIA and type IIIB, age, AO/OTA type 42C3, a crush injury, and a fall were the strongest predictors of infection in this model (Fig. 2). Online Prediction Tool The final model was incorporated in an online open-access multiplatform prediction tool, allowing users to calculate the probability of infection in patients after operative treatment of TABLE IV Performance of ML Algorithms in Predicting Subsequent Surgery in the Validation Cohort* Model AUC Calibration Slope Calibration Intercept Brier Score† Bayes point machine 0.82 1.04 0.04 0.078 Boosted decision tree 0.80 0.83 0.11 0.083 Penalized logistic regression 0.81 1.07 0.09 0.079 *N = 364. †Upper limit of the Brier score = 0.085. Fig. 2 Importance of the variables in the final model that was based on the penalized logistic regression algorithm. 536 TH E JOURNAL OF BONE & JOINT SURGERY d JBJS . ORG VOLUME 103-A d NUMBER 6 d MARCH 17, 2021 MACHINE LEARNING ALGORITHM FOR TIBIAL SHAFT FRACTURES AT RISK FOR INFECTION AFTER SURGERY TSFs (https://traumaplatform-ai-prediction-tools.shinyapps.io/ tibia-shaft-infection/). Figure 3 displays the results that were generated by the prediction tool for 3 case scenarios.''', '''5. Results In semantic relatedness, all relationships between words like is-a, part-of, contains, has etc defined by global ontologies like WordNet, SUMO and ConceptNet[26] are considered. Accuracy is computed as correlation value of human judgment and similarity computed by the similarity measure in Eqn.(1). Higher correlation signifies higher accuracy. Pearson correlation is used [27] to compute the accuracy. The results are computed for the terms in Solar domain. Various concept pairs in solar domain such as (Planet & Earth), (Earth & Sun) are identified and the similarity among these solar domain words pair is manually computed between range from [0-4] shown in table 5. Table 5 Similarity computation in Solar domain Concept 1 Concept 2 Human Similarity [0-4] Proposed method Similarity Planet Sun 3.6 0.844 Sun Earth 3 0.71 Planet Moon 3 0.69 Earth Venus 2.9 0.64 Planet Jupiter 3 0.7 Planet Spacecraft 2.6 0.58 Voyager Solar 3.2 0.69 Planet Voyager 2.6 0.47 Moon Comet 2.5 0.45 Planet Asteroid 3.5 0.94 Jupiter Moon 3.6 0.707 Jupiter Saturn 3.4 0.801 The accuracy is calculated by finding the Pearson Correlation between the human similarity and Proposed method 1108 Shivani Jain et al. / Procedia Computer Science 167 (2020) 1102–1109 similarity. We got an accuracy of 0.863 in solar domain. In literature, three standard datasets are used for accuracy assessment. Miller & Charles [28], Robenstein & Goodenough [29] and 353 word similarity[30]. The results are compared with other measures in these three databases and shown in table 6. It is found that our proposed method results are better than previously defined measures. Table 6 Comparison among different Measures Measures Type M&C R&G 353 word similarity Path length[31] Edge –Method 0.59 N/A N/A Wn& Palmer [32] Edge –Method 0.74 N/A N/A Lin[11] IC(Corpus) 0.7 0.72 N/A Jiang &Conrath[33] IC(Corpus) N/A 0.82 N/A Tversky[13] Feature 0.71 N/A N/A Feature based Approach using Wikipedia[14] Feature 0.78 0.8 0.83 Feature based approach[2] Feature based 0.82 0.83 0.82 Association calculation[34] Graph 0.84 0.83 N/A Semantic Relatedness[12] Edge+ IC 0.79 0.81 0.83 Similarity(our approach) LSA+ Feature 0.86 0.84 0.853''', '''4. Experimental evaluation This section presents our experimental evaluation. First, we present our experimental setup and data sets used for the experiments. Then, we present the computation of descriptive statistics and statistical tests. For machine learning models, we compare our proposed solution with Spark, a popular Hadoop big data system, and parallel DBMS, to make sure our solution is competitive with other parallel systems. We also present the trade-off between computation in a single machine and a parallel cluster. All the time measurements were taken five times and we report the average excluding the maximum and minimum value. The standard deviations are very small and the highest standard deviation recorded is 2.03 s. 4.1. Experimental setup 4.1.1. Hardware and software We performed our experiments using an 8-node parallel cluster each with Pentium(R) Quadcore CPU running at 1.60 GHz, 8 GB RAM, and 1 TB disk space. We choose R as a representative of data science language and we develop our solution using R and C++. We used the standard UNIX commands to split and transfer the data set among the processing nodes. For parallel comparison, we used the Spark-MLlib library for comparing with Spark, and we used Vertica to compare with a parallel DBMS. In the case of Spark, we programmed the models using Scala, for Vertica, we used a previous solution developed with UDFs and SQL queries. 4.1.2. Data sets Machine learning or statistical models does not work well on raw data as it may have noise, missing values, outlier values, and so on which can overfit or underfit the models. Also, large public data sets for computing all the models are not available. Therefore, we had to use common data sets available and replicate them to mimic large data sets. We used two data sets presented in Table 2 as our base data sets: YearPredictionMSD and CreditCard data set, obtained from the UCI machine learning repository. We sampled and replicated the data sets in random order to get varying 𝑛 (data set size). And for lower 𝑑, we chose it randomly from the original data set. 4.2. Computing descriptive statistics on data subsets As mentioned in Section 3.2, we can compute descriptive statistics and statistical tests utilizing our summarization matrix. We can get subsets of the original data set based on some filter (e.g. gender, age group, location) and see the descriptive statistics there before applying any machine learning models. Here, Table 3 shows how our summarization performs on data subsets compared to the original data set in parallel 𝑁 = 8 machines. We take the YearPrediction data set and generate data subsets from there. We generate 10 data subsets (YearPrediction-Subset1) and 2 data subsets (YearPrediction-Subset2) of equal sizes from the original data set. We report the time of one representative data subsets from each of them (the other subsets have almost the same time measurements, within 1 s variation) and also for the full data set. For each of them, the time for Phase 0, Phase 1, and Phase 2 from Section 3.3 is reported in Table 3. From Phase 0 and Phase 1, we can see that our solution scales well and there is almost no performance penalty regardless of data subset or the full data set. As for Phase 2, we compute the descriptive statistics in one machine for data subsets (Eq. (7)) using the summarization matrix. For that, we first need to send the partial summarization matrices to the master node, add them, and then compute the statistics. Computing the statistics is fast utilizing R run-time and is done in less than a second (< 1 s) and added to get the final summarization matrix for each subset (total ∼ 8 s). Based on this matrix, we perform the mean comparison test. First, we get the mean, std. deviation, and total number of points from the final summarization matrix of each subset (Eq. (9)) and then perform the test using Eq. (8). As we are computing this part in R run-time, it is fast and takes less than 1 s for each subset. 8 S.T. Al-Amin and C. Ordonez Data & Knowledge Engineering 136 (2021) 101930 Table 4 Time (in Seconds) to compute mean comparison on data subsets in parallel 𝑁 = 8 machines (M = millions). Data set 𝑑 𝑛 Partition Compute 𝛤 Stat test YearPrediction-subset3 90 1M 18 14 9 YearPrediction-subset4 90 9M 199 122 9 YearPrediction-subset5 90 5M 102 67 9 YearPrediction-subset6 90 5M 102 67 9 Table 5 Time (in Seconds) to compute the ML models with our solution and in Spark (𝑁 = 8 machines; M = Millions). 𝛩 R (𝑁 = 8) Spark (𝑁 = 8) (Data set) 𝑛 𝑑 Partition Export 𝛤 + 𝛩 Partition 𝛩 LR 1M 10 9 6 12 7 41 (Year- 10M 10 23 13 29 17 286 Prediction) 100M 10 317 96 218 161 1780 PCA 1M 10 9 6 12 7 15 (Year- 10M 10 23 13 29 17 46 Prediction) 100M 10 317 96 218 161 277 NB 1M 10 11 6 13 7 Crash (credit- 10M 10 28 17 36 25 Crash card) 100M 10 335 125 252 231 Crash KM 1M 10 11 6 13 7 64 (credit- 10M 10 28 17 36 25 392 card) 100M 10 335 125 252 231 Stop 4.3. Comparison with Hadoop parallel big data system: Spark Here, we compare our solution with Spark for machine learning models, a popular parallel data processing engine developed to provide faster and easy-to-use analytics. For that, we partition the data sets using HDFS and then run the models using the Spark-MLlib library (with Scala), Spark’s scalable machine learning library to run the ML models. We emphasize that we used the recommended settings and parameters as given in the library documentation. Here, we are taking the data sets with a higher 𝑛 (𝑛 = 1𝑀, 10𝑀, 100𝑀) and medium 𝑑 (𝑑 = 10) to demonstrate how large data sets perform on both. Moreover, we do not assume that the data set is already in the processing machines. As mentioned in Section 3.3, we assume data to be analyzed can be stored in the (1) disk of a large machine (local file system), (2) HDFS, or (3) already partitioned in the processing machines. If data is in the file system, we need to partition the data set among 𝑁 machines. Nowadays, data can be also in the HDFS (or cloud) as it is a popular platform to store huge data sets. In that case, we have to export the data and then partition it among 𝑁 machines. It is possible to read directly from the HDFS using some library but we are not exploring that here. Finally, if the data set is already partitioned in the processing machines, no partitioning is needed. We do not show similar experiments based on data storage for descriptive statistics and statistical tests as it would be redundant and trivial. Table 5 presents the time to compute the ML models in the parallel cluster with our solution and Spark. For each entry, we round it up to the nearest integer value. From Table 5, the ‘Partition’ column is the time to partition 𝑋 among 𝑁 processing nodes. This is Phase 0 from our 3-phase algorithm discussed in Section 3.3. We used the standard and fastest UNIX commands available to perform this operation. The ‘Export’ column is the time to export the data set 𝑋 from HDFS to the local machine. And the ‘𝛤 + 𝛩’ column is the time to compute 𝛤 in parallel 𝑁 machines, send them to the master node to compute the final summarization matrix, and compute the machine learning model (𝛩) from it. This process is Phase 1 and 2 combined from our algorithm in Section 3.3. In the Spark part of Table 5, ‘Partition’ is the time to load and distribute the data set in HDFS among 𝑁 machines. And we report the time to compute the models using Spark-MLlib in the ‘𝛩’ column. Fig. 2 shows the total time based on Table 5 to compute the ML models using different approaches discussed above. We simply add the times to get the total time for different approaches. We can see that if the data set is already partitioned in the processing machines, computing the models utilizing our summarization matrix is fast in all cases. Partitioning the data set first and then computing the ML models is a bit slower, but still fast. Exporting the data set from HDFS adds more time over the previous approach. On the other hand, Spark is mostly slow compared to any of our approaches. For Linear Regression, Spark minimizes the specified loss function with regularization. For PCA, the Spark-MLlib library uses a similar algorithm as ours. It computes 𝑋𝑇 ∗ 𝑋 for large 𝑋 by computing the outer product of each row of the matrix by itself, then adding all the results up. This is 𝑄 from our summarization matrix which Spark manipulates in the main memory by each worker node. Still, Spark is slightly slower than our method for computing only the PCA model. For Naïve Bayes, Spark-MLlib implements the multinominal Naïve Bayes whose major drawback is having negative values in the data set crashes the model. As Spark crashes showing ‘‘illegalArgumentException’’ during the execution of NB, there is no plot for Spark in Fig. 2(c). And for K-means, Spark implements a parallelized variant of k-means++ [8] which generates a k-means model and is roughly 𝑂(𝑘), so this suffers a slower start with a large 𝑘. Also, it is expensive when the model is trained. If we analyze the plots from Fig. 2 more carefully, we can see that when the data set is already partitioned, computing the models is at least 2𝑋 faster than our other approaches. In the other approaches, we have to partition the data set (data in disk), or 9 S.T. Al-Amin and C. Ordonez Data & Knowledge Engineering 136 (2021) 101930 Fig. 2. Total time (in Seconds) to compute ML models using different approaches (M=millions). export from HDFS and then perform partition it (data in HDFS). This is slower because we are partitioning the text (.csv) files, not binary files. This is a bottleneck and takes a long time. However, it is due to the file format and not a shortcoming of our solution. Moreover, R can read binary files and as we can call C++ code from R, it is possible to read binary files efficiently in R (but CSV is most common). However, there are some drawbacks to our solution. If some distribution cannot be summarized with sufficient statistics from the Gaussian distribution, then our approach would not work. For example, exponential distribution may be inaccurate. But, in general, one or multiple Gaussian works well. Also, it is not possible to get the original data set back from the summarization matrix. That is, we cannot get 𝑋 back from 𝛤 or 𝛤 𝑘 . 4.4. Comparison with a parallel big data system: DBMS Now, we compare our solution with a parallel columnar DBMS (Vertica) running on 𝑁 processing nodes. In general, parallel columnar DBMS performs better than row DBMS when 𝑑 is not large. However, it is feasible that a parallel row DBMS may be faster, but 𝑑 would have to be very high, probably hundreds of columns and a dense matrix. We adapted the solution presented in [2] using UDFs and SQL queries which is the current best solution to compute the summarization matrix in a parallel DBMS. As there is no prior solution of 𝛤 𝑘 matrix in a DBMS, here we only compare our solution with the 𝛤 matrix. We already know that the machine learning model (𝛩) computation is very fast (∼1 s) in the main memory exploiting 𝛤. So, we only report the time to compute the 𝛤 using 𝑁 processing nodes which are shown in Fig. 3. We compute 𝛤 for varying 𝑛 = 1𝑀, 10𝑀, 100𝑀 and 𝑑 = 10 in two cases: (1) when data is not partitioned (data in disk or HDFS), and (2) when data is already partitioned. We consider both cases to give the parallel DBMS a fair chance as it is often assumed for analytics that data is already stored in the DBMS. Fig. 3(a) shows the comparison when data is in the disk. In this case, we split the data set into 𝑁 processing nodes and then compute 𝛤. And, if the data set is in the HDFS, then we export the data first and then partition it as mentioned above. For DBMS, we used standard SQL queries to COPY (Partition) the data set in all machines. As partitioning in DBMS is slow, we can see that parallel DBMS performs much slower in all cases than our solution when it has to partition the data first. On the other hand, Fig. 3(b) shows the comparison to just compute 𝛤 using 𝑁 machines where the data set is already partitioned and loaded into DBMS. Our solution also performs better for 𝛤 computation as 𝑛 grows (Fig. 3(b)). Also, DBMS solutions using UDFs are not portable and they require a lot of memory to scale up. 10 S.T. Al-Amin and C. Ordonez Data & Knowledge Engineering 136 (2021) 101930 Fig. 3. Time (in Seconds) comparison for 𝛤 on 𝑁 = 8 nodes: our solution vs parallel DBMS for varying 𝑛 and 𝑑 (M=millions). Fig. 4. Time (in Seconds) comparison for 𝛤 and 𝛤 𝑘 in a parallel cluster (𝑁 = 8 machines) and a single machine (𝑁 = 1 machine) for varying 𝑛 and 𝑑. 4.5. Understanding trade-offs: Parallel cluster and single machine Here, we understand the trade-offs of computing our summarization matrix on a parallel cluster and a single machine. Though we have seen that parallel processing accelerates the computation, we may not need a large cluster each time, especially when the data set size is smaller. Instead, we can use a single machine to handle small data sets as parallel processing may introduce overhead and make the processing slower. Fig. 4 shows the comparison to compute 𝛤 and 𝛤 𝐾 in one machine and parallel (𝑁 = 8) machines. We can see that a single machine performs better when 𝑛 and 𝑑 is low (≤ 1𝑀 × 10) in both cases. The reason is, the parallel cluster is spending much time partitioning the data set and transferring the partial 𝛤𝐼 (or 𝛤 𝑘 𝐼 ) matrices. On the other hand, parallel cluster seems to be faster from 𝑛 = 1𝑀 and 𝑑 = 20. When 𝑛 is very high (𝑛 = 10𝑀 or more), the parallel cluster is at least 2𝑋 to 4𝑋 faster than the single machine and becomes the obvious choice for processing. This is because a single machine cannot scale as the data size grows due to limited memory. However, we should emphasize that these time measurements are only for partitioning the data set and computing the actual summarization matrix (Phase 0 and 1 from Section 3.3), and it does not include the time to compute the machine learning or statistical models (Phase 2 from Section 3.3).''', '''7. Discussion and analysis Figure 1 shows the trends of research in the various applications year-wise. How the research in MSR has increased from 2010 to April 2018 can be observed. Sådhanå (2019) 44:244 Page 9 of 17 244 • Out of 257 papers, approximately 48 percent of the papers belong to summarization and sentiment analysis. Sentiment analysis and summarization are the most popular topics of research in the NLP field. Sentiment analysis also plays a very important role while performing summarization task. • There is a huge increase in research works in mobile analytics from 2012 to 2015. In this survey, we have considered only the papers that involve NLP technique to perform mining-related task. The percentage increase of the works in this area is too much if we consider other techniques as well. • Research in the field of extractive summarization [62] has reached the saturation stage after 2013 but now the research is going on in the field of abstractive summarization. • Research in the field of sentiment analysis is increasing year by year. • Norms mining is still a very young field of research in MSR. Only 16 papers have been published till now, presenting opportunities for research in this field. 8. Future scope and challenges Here, we discuss the future research areas and the challenges that we have identified from various papers published in the recent years related to works in software repositories. 8.1 Sentiment analysis It is an ongoing research topic among researchers. Lot of work has been done in this field on social sites, questionanswering sites, etc. However, from the literature review, it has been found that there is a lack of research in the field of context-based sentiment analysis. SentiStrength, NLTK and Stanford NLP are the three dominant sentiment analysis tools available today. However, studies have shown that these tools do not give good results when applied to software engineering artefacts due to domain-related technical terms, which are frequently used in software engineering context. There are many terms in software engineering like ‘‘super’’, ‘‘resolve’’, ‘‘dead’’ and ‘‘block’’ that do not hold any sentimental polarity in domain-terms but in English dictionary, they hold sentimental polarities. Resolving these issues in this field of research is still an open problem [56]. Distinguishing the context-sensitive meanings of words and negation handling in software engineering domain is also a big challenge. There is a lack of sentiment-annotated text in software engineering for research in this field [56]. Detection of irony and sarcasm hidden in text is still an open problem in this field of research. Tourani et al [47] found that currently available sentiment analysis tools do not give good precision due to the presence of ambiguous technical terms and the difficulty of separating the neutral technical e-mails from positive and negative e-mails. They addressed the need of off-shelf tools for the purpose of sentiment analysis in software engineering context. Lin et al [125], in their Table 3. Keywords used for selecting the research papers. Application Strings used Sentiment analysis Sentiment analysis in software repositories, sentiment analysis, sentiment analysis in software engineering Summarization Summarization of software artefacts, summarization in software engineering, summarization of software code, summarization in software Traceability Traceability in software engineering, traceability in software repositories Mobile analytics Mobile app store mining, MSR, MSR with mobile apps, mobile app store analytics, mobile software repositories with mobile apps, summarization in mobile analytics, sentiment analysis in mobile apps Norms mining Norms mining, norms mining in software repositories, norms software Figure 1. Papers distribution according to the application: year-wise. 244 Page 10 of 17 Sådhanå (2019) 44:244 paper, addressed the concern of sentiment analysis tools in software engineering context. They analysed the results of these tools on stackOverflow sentences, comments on issue tracking sites and mobile apps, and found that the tools worked a little better with mobile app reviews and issue tracking sites but gave very less precision with stackOverflow sentences. 8.2 Norms mining in open-source development communities Norms are very important for smooth functioning of a large diversified project, as they help resolve the behavioural issues among the team. From our survey, there has not been much research in this area. Many questions like how the norms differ from small projects to large projects, and how norms and the role of a person are related to each other, needto be considered; finding out under what conditions the norms emerge, how cultural differences and norms are related, how the norms adoption and compliance differ between open-source and closed-source software projects, impact of norms on stakeholders and why some norms are adopted efficiently and not some others [93, 98] need to be researched more. NLP plays an important role in norms mining as the human-interaction data is mostly noisy, incomplete and contains lot of domainspecific slangs, etc. NLP helps find the meaning of sentences and phrases, and the relationship between the words and sentences that help extract the norms. 8.3 Mobile analytics Even though the mobile user’s review data is very useful for developers, it is very massive. The reviewers talk about multiple topics and, many times, posts are incomplete [87]. In addition, the use of abbreviations, incorrect grammar, unstructured and informal nature, use of slangs and typomistakes create challenges during preprocessing of this data [82]. Handling this massive data to find out which data is important is another challenge for the developers. 8.4 Summarization techniques A lot of work has been done in the field of summarization in the software engineering context. However, the questions like ‘‘what is a good summary?’’, ‘‘how much information should be added to a summary?’’, ‘‘how should a summary be evaluated?’’, etc. are still open questions in this field of research. There are only a few quantitative evaluation measures, like ROUGE-score and Pyramid-score, available to calculate the informativeness of the summaries. Pawar and Mago [94] used the deep semantic analysis to find the effectiveness of summaries, but it was time-consuming. There is a need for finding better ways to evaluate the summaries that not only consider the informativeness of the summaries but also various aspects like semantics, contradiction, pronoun resolution, paraphrases, etc. Not just the evaluation of summaries is a big issue but successful application of NLP techniques to create abstractive summaries is also a big challenge. There has been a lot of research in the field of extractive summarization as it is easy to perform, but research in the field of abstractive summarization is still lacking as it is very challenging because of the complexity of NLP [95]. Proper handling of pronouns considering the context, capturing of the overlapping information and sentence segmentation in conversation-related artefacts are still open problems. Representation of abstractive summarization is also one of the challenges in this field [61]. Heterogeneity [72], incompleteness of code fragments [97] and the multi-dimensional nature of software artefacts pose other challenges in the field of summarization of software artefacts. Investigations show that abstractive summarization is more promising than extractive summarization [95]. Haiduc et al [96] showed that abstractive summarization performed better than extractive summarization. Most of the summarization techniques have been applied to textual data. However, most of the software engineering data consists of heterogeneous data consisting of code fragments from different languages, and lot of other data apart from textual data. More research is required that considers other aspects of text as well, like XML data or code snippets, which are generally embedded in the software documents [72]. Even the automatic generation of documentation for software artefacts like automatic generation of release notes, source code, commit notes, etc. is not fully explored. For example, ARENA is one approach that has been developed for release notes generation but there are many opportunities, like deciding what has to be included to the summary and what not to include, how the release notes Table 4. Paper distribution: source-wise. Link No. of papers Summarization papers Sentiment analysis Traceability Mobile analytics Norms mining Springer 65 24 15 2 24 6 IEEE 87 31 25 22 9 7 ACM 70 12 14 22 22 1 Others 24 11 4 3 4 2 Total papers found 78 58 49 56 16 Sådhanå (2019) 44:244 Page 11 of 17 244 Table 5. Summary of studies. Author Artefact Application Corpus NLP technique Schugerl et al [39] Bug reports Bug repositories processing (quality of bug report) AgroUML [101] POS tagging, chunking, emotions identification Runeson et al [34] Bug reports Bug repositories processing (duplicate bug reports detection) Sony Erickson Mobile Communications Tokenization, stemming, stop words removal, vector space representation, similarity calculation Surekha and Jalote [40] Bug reports Bug repositories processing (duplicate bug reports detection) Eclipse project [106] n-gram model, spelling correction, abbreviation expansion, stemming, hyphen-removing, similarity calculation Minh [41] Bug reports Bug repositories processing (duplicate bug reports detection) AgroUML [101], SVN [107] and Apache n-gram model, similarity calculation Banerjee et al [43] Bug reports Bug repositories processing (duplicate bug reports detection) Eclipse [106] and Firefox [108] Common sequence matching, strings matching Banerjee et al [42] Bug reports Bug repositories processing Firefox [108] Tokenization, stemming, stop word removal, Levenshtein distance analysis, neighbour word analysis, likelihood analysis Alobaidi and Mahmood [73] Requirement artefacts Traceability NASA Moderate Resolution Spectrometer (MODIS), Borland CaliberRM dataset Lexical relations (synonymy, hypernymy, etc.), similarity calculation between concepts Murgia et al [54] Bug reports Sentiment analysis Apache Software Foundation [22] Emotions identification Guzman et al [57] Commit messages Sentiment analysis GitHub [2] Lexical approach using sentiStrength Islam and Zibran [56] Issue comments Sentiment analysis JIRA [109] Dictionary, lexicon-based approach (built sentiStrength-SE over sentiStength) Li et al [71] Source code Summarization (unit test case generation) GitHub [2] Natural language generation, semantic analysis to identify the focal methods in source code 244 Page 12 of 17 Sådhanå (2019) 44:244 Table 5 continued Author Artefact Application Corpus NLP technique McBurney and McMillan [63] Source code Summarization Nano-XML [110] Software Word Usage Model (call graph generator), PageRank, natural language generation Shen et al [45] Source code Summarization (commit messages) JGit [111] Abstract syntax tree, rules, tree differencing algorithms Le et al [15] Source code Traceability Commons CLI [112], Commons IO [113], Commons Collections [113], Commons Math [114], Commons Lang [115], Commons CSV [116] Features extraction Ponzanelli et al [117] QA sites Summarization StackOverflow [7] Extended LexRank, island parser, abstract syntax tree, similarity calculation Guerrouj et al [67] Source code Summarization StackOverflow [7] Island parsing, document pre-processing, n-gram model, similarity calculation Rastkar et al [17] Source code Summarization jHotDraw [118], jEdit [119] RDF graph, Verb-DO, similarity calculation Liu et al [88] Mobile app reviews Sentiment analysis ? summarization Google Play Android App Store (beautiful widgets’’ [120] and ‘‘Where is my Perry’’ [121]) POS tagging, sentiment rules, parsing, feature-based extraction Sridhara et al [1] Source code Summarization Megamek [122], jHotDraw [118], Jajuk [123], SweetHome3D [124] Abbreviation expansion, identifiers splitting, abstract syntax trees, CFGs, software word usage model (SWUM), natural language generation Ahmed et al [46] Source code Sentiment Analysis 20 popular OSS projects Lexicon analysis, stop word removal, stemming, regular expression matching, dictionary, bagof-word representation, chunking, POS tagging Calefato et al[59] QA sites Sentiment Analysis stackOverflow [7] Lexicon-based approach, word embeddings, keyword-based features Bazelli et al [52] QA sites Sentiment analysis stackOverflow [7] Linguistic inquiry and word count Vu et al [84] Mobile app store Mobile analytics (mining user opinions) Google Play Word2Vec, similarity calculation, text preprocessing Sådhanå (2019) 44:244 Page 13 of 17 244 should be presented to users, [16] etc. Most of the source code summarization techniques are focused on C?? and Java languages. More work on performing summarization for other object-oriented languages is required [62]. CrowdSourcing is one of the emerging ways of collecting information for summarization, which helps in evaluation of summaries. CrowdSourcing in the field of software summarization has been used in very small scale. More efforts on extending it are required [97]. More work on creating personalized summaries based on the role of persons is required [62]. There are not many designers of automatic documentation tools, as what should be the good characteristics for generating high-quality summaries is not yet very clear [95]. More work on creating the unit test cases summaries is required. Application of unit test case summarization on test case generation tools needs more research [71].''', '''5. RESULTS The experiments are carried using data collected from Blackboard, Facebook, and WhatsApp over the course of 5 months (1 semester) and for four different courses. A generalpurpose lexical analyzer is designed to perform the sentiment analysis and identify known polarity sentiment words. In SALE (Sentiment Analysis Lexicon for English), an algorithm is employed that assigns a unified score between −1 and +1 to each word. A positive score means that the word is subjectively positive whereas a negative feedback carries a negative score while the absolute value indicates sentiment strength. Firstly, we defined seed words that may express sentiments specific to the topic of teaching, discarding other topics such as food, fun, and movies. Each word is assigned three scores containing positive, negative, and neutral values. A numeric value between zero and one is assigned to its positive and negative polarities. Words are compared with the provided data list, and the corresponding polarity is marked if there is a match, while a neutral score is assigned otherwise. Words may have more than one meaning, and the associated score needs to be determined based on usage and context. Their different scores are combined to an average. The interpretation of data is reflected in four measures containing knowledge and understanding, contents of the course, teaching style, and assessment. During 6 months, social media data collection, keywords, and seed words related to course review and feedback are processed, and graphs related to positivity or negativity for the course subjects are prepared. At the end of the semester and during the final examination, written survey forms and questionnaires are distributed among the Section C: Computational Intelligence, Machine Learning and Data Analytics The Computer Journal, Vol. 65 No. 4, 2022 Downloaded from https://academic.oup.com/comjnl/article/65/4/918/5921713 by Rijksuniversiteit Groningen user on 28 April 2024 Semantic Analysis to Identify Students’ Feedback 923 FIGURE 3. Automated algorithm results using semantic analysis. FIGURE 4. Manual survey forms and questionnaire results. students. The survey forms are designed on the same four measures as were analyzed using social media platforms. The results are presented in Figs 3 and 4 and Table 1. The first parameter, knowledge, and understanding show a student’s understanding of the course objectives and learning outcomes of the course. In the second measure for course contents, a student assesses the course prerequisites and topics in the current course according to the relevance to the course for his chosen program. In the teaching method, a student marks the presentation and teaching style of the course presenter. In the assessment, the student evaluates and provides feedback on various assessment styles offered by the course presenter. As shown in Table 1, survey form results and automated analysis follow the same pattern. In survey forms, students have to rate four parameters giving grades between 1 and 5. A score of 1 corresponds to 20% or minimal satisfaction whereas a score of 5 corresponds to 100% satisfaction. The average score in the student surveys for knowledge and understanding (K&U) for all courses is 76%. In the automated analysis, the score for K&U is 71%. The major difference in the results was observed for teaching methodology. In the survey forms, it obtained more than 80%, while in the automated analysis, it did not even reach 70%. Although survey forms are blind as students do not write their names on the forms, they may still be afraid in the presence of the teacher. In social media, they are free to express their opinions and views about the teacher and the course. Hence, our automated analysis represents a more accurate student feedback and can be effectively used to improve the curriculum and teaching methodologies. In Table 1, students gave the Computer Architecture course an 80% score for knowledge and understanding, equivalent to 4 out of 5. Based on the analysis of social media data, it received only 70% which reflects an ongoing discussion during the entire duration of the semester while survey forms are completed only once at the end of a semester. Initially, the course knowledge is not clear, but by the end of the semester, understanding is better that may result in increased marks in the survey forms. Students were generally not happy with the exam results and complained about their marks on social media to reflect their anger. The average score for the exams assessments is 65% in social media and 70% in the manual end-of-term surveys. Using online examination tools and other assessment procedures, teachers can improve the quality of their assessments and enhance transparency in examinations [31].''', '''3. Results 3.1. Sentiment analysis accuracy in U.S. financial news In this study, we used LLMs to analyse sentiment in U.S. financial news. We processed a dataset of 965,375 articles from Refinitiv, spanning from January 1, 2010, to June 30, 2023. We used 20% of these articles as a test set. We measured the accuracy of each model in predicting the direction of stock returns based on news sentiment. This accuracy indicates how well the model links the sentiment in financial news with stock returns over a three-day period. We evaluated four models: OPT, BERT, FinBERT and the Loughran-McDonald dictionary. Their performance in sentiment analysis is shown in Table 3. Table 3 Language model performance metrics: accuracy, precision, recall, specificity, and the F1 score for each model. The OPT model is the most accurate, followed closely by BERT and FinBERT. Metric OPT BERT FinBERT Loughran-McDonald Accuracy 0.744 0.725 0.722 0.501 Precision 0.732 0.711 0.708 0.505 Recall 0.781 0.761 0.755 0.513 Specificity 0.711 0.693 0.685 0.522 F1 score 0.754 0.734 0.731 0.508 The results show that the OPT model is the most accurate, followed closely by BERT and FinBERT. The Loughran-McDonald dictionary, a traditional finance text analysis tool, has significantly lower accuracy. This indicates that language models like OPT, BERT, and FinBERT are better at understanding and analysing complex financial news. The precision and recall values further support the superiority of the OPT model; its F1 score, which combines precision and recall, also confirms its effectiveness in sentiment analysis. These findings confirm that language models, particularly OPT, are valuable tools for analysing financial news and predicting stock market trends. 3.2. Predicting returns with LLM scores This section assesses the ability of various LLMs to predict stock returns for the next day using regression models. Our regression with Eq. (1) uses LLM-generated scores from news headlines as the main predictors. To account for unobserved variations, these regressions include fixed effects for both firms and time, and we cluster standard errors by date and firm for added robustness. Table 4 provides our regression findings, focusing on how stock returns correlate with predictive scores from advanced LLMs, specifically OPT, BERT, FinBERT and the Loughran-McDonald dictionary models. Our findings reveal the predictive capabilities of the advanced LLMs. The OPT model, in particular, demonstrates a strong correlation with next-day stock returns, as indicated by significant coefficients in different model specifications. The FinBERT model follows closely, showcasing its own robust predictive power. BERT scores, while more modest in their predictive strength, still show a statistically significant relationship with stock returns. We also observe that the predictive strength increases when both LLMs are used as independent variables in the same regression. In contrast, the Loughran-McDonald dictionary model exhibits the least predictive power among the models examined. Our analysis suggests that several factors contribute to explain the different performance among OPT, BERT and FinBERT, notably model design, parameter scale, and the specificity of training data. OPT’s expanded parameter space, exceeding that of BERT and FinBERT, alongside its advanced training methodologies, is likely to cause its superior forecasting accuracy in stock returns and portfolio management. Furthermore, the nuanced performance of FinBERT, despite its financial domain specialization, raises intriguing considerations. Our exploration detailed in Section 3.3 posits that the broader pre-training data diversity of BERT and the potential for overfitting in highly specialized models such as FinBERT might explain this unexpected outcome. These insights collectively emphasize the intricate balance between model specificity, scale, and training regimen in optimizing predictive performance within financial sentiment analysis. The robustness of our regression models is further underscored by the inclusion of a substantial number of observations, ensuring a comprehensive and representative analysis. Additionally, the adjusted 𝑅-squared values, while moderate, indicate a reasonable level of explanatory power within the models. The reported AIC and BIC values aid in assessing model fit and complexity, further enriching our comparative analysis across different LLMs. Finance Research Letters 62 (2024) 105227 6 K. Kirtac and G. Germano Table 4 Regression results of stock returns on LLM sentiment scores done with Eq. (1), which includes firm and time fixed effects (FE) represented by 𝑎𝑖 and 𝑏𝑛 . The independent variable 𝑥𝑖,𝑛 includes prediction scores from the language models. This analysis compares scores from OPT, BERT, FinBERT and Loughran-McDonald dictionary models, providing insights into their predictive abilities for stock market movements based on news sentiment. This analysis encompasses all U.S. common stocks with at least one news headline about the firm. 𝑇 -statistics are presented in parentheses. Regressions 1 and 2 include two scores, regressions 3–6 only one. * 𝑝 < 0.05. ** 𝑝 < 0.01. *** 𝑝 < 0.001. Regression 1 2 3 4 5 6 OPT score 0.274*** 0.254*** (5.367) (4.871) BERT score 0.142** 0.091* 0.129* (2.632) (1.971) (2.334) FinBERT score 0.257*** 0.181*** (5.121) (4.674) LM dictionary score 0.083 (1.871) Observations 965,375 965,375 965,375 965,375 965,375 965,375 R2 0.221 0.217 0.195 0.145 0.174 0.087 R2 adjusted 0.183 0.184 0.195 0.145 0.174 0.087 R2 within 0.021 0.022 0.017 0.009 0.016 0.002 R2 within adjusted 0.020 0.020 0.017 0.009 0.016 0.002 AIC 64,378 77,884 62,345 97,473 67,345 135,783 BIC 117,231 132,212 115,655 114,746 109,272 123,382 RMSE 5.32 11.12 4.21 14.12 9.75 23.54 FE: date X X X X X X FE: firm X X X X X X 3.3. Performance of sentiment-based portfolios Next, we assess the effectiveness of sentiment analysis in portfolio management by constructing various sentiment-based portfolios, including market value-weighted portfolios. These portfolios are developed using sentiment scores derived from different language models: OPT, BERT, FinBERT, and the Loughran-McDonald dictionary model. The investment strategies employed in our analysis can be described as follows: each LLM is utilised to create three distinct portfolios, one composed of stocks with top 20 percentile positive sentiment scores (long), another comprising stocks with top 20 percentile negative sentiment scores (short), and a self-financing long-short portfolio (L-S) based on both top 20 percentile negative and positive scores. Additionally, we include benchmark comparisons with value-weighted and equal-weighted market portfolios without considering sentiment scores. Valueweighted portfolios distribute investments based on the market capitalisation of each stock, while equal-weighted portfolios allocate investments equally to all stocks, regardless of market capitalisation. The selection of value-weighted and equal-weighted market portfolios was made to align with passive trading strategies, a widely acknowledged method in financial research (Fama and French, 1993; Carhart, 1997). We evaluate these strategies using key financial metrics, including the Sharpe ratio, mean daily returns, standard deviation of daily returns, and maximum drawdown. The long-short OPT strategy demonstrates the most robust risk-adjusted performance, as evidenced by its superior Sharpe ratio indicated in Table 5. The Loughran-McDonald dictionary model-based strategy (L-S LM dictionary) laggs behind, particularly when compared to the value-weighted market portfolio. This highlights the varying effectiveness of different sentiment analysis models in guiding investment decisions and underscores the significance of model selection in sentiment-based trading. Table 5 Descriptive statistics of trading strategies. The table presents the Sharpe ratio, mean daily return (MDR), daily standard deviation (StdDev) and the maximum daily drawdown (MDD) for the trading strategies based on the sentiment analysis models OPT, BERT, FinBERT, and Loughran-McDonald dictionary (LM dictionary), each comprising long (L), short (S), and long-short (L-S) portfolios. The portfolios are value-weighted for comparison to a value-weighted (VW) market portfolio, which is provided for benchmarking, as well as an equal-weighted (EW) portfolio. LM dictionary refers to a sentiment analysis approach that uses a dictionary of finance-specific terms developed by Loughran and McDonald. OPT BERT FinBERT Long Short L-S Long Short L-S Long Short L-S Sharpe ratio 1.81 1.42 3.05 1.59 1.28 2.11 1.51 1.19 2.07 MDR (%) 0.32 0.25 0.55 0.25 0.21 0.45 0.22 0.18 0.39 StdDev (%) 2.18 2.91 2.49 2.49 3.19 2.68 2.59 3.31 2.81 MDD (%) −14.76 −24.69 −18.57 −17.89 −27.95 −21.95 −19.71 −29.94 −23.82 LM dictionary EW VW Long Short L-S Long Short L-S Long Short L-S Sharpe ratio 0.87 0.66 1.23 1.25 1.05 1.40 1.28 1.08 1.45 MDR (%) 0.12 0.13 0.22 0.18 0.15 0.33 0.19 0.16 0.35 StdDev (%) 3.54 4.13 3.74 2.90 3.70 3.20 2.95 3.75 3.25 MDD (%) −35.47 −45.39 −38.29 −31.13 −42.21 −32.87 −28.76 −38.95 −31.87 Finance Research Letters 62 (2024) 105227 7 K. Kirtac and G. Germano Fig. 1. Cumulative returns from investing $1 with value-weighted, zero-cost long-short portfolios based on OPT (red), BERT (yellow), FinBERT (dark blue) and the Loughran-McDonald dictionary (green), rebalanced daily with a 10 bps transaction cost. For comparison, we also show a value-weighted market portfolio (light blue) and an equal-weighted market portfolio (orange), both without transaction costs. Finally, we examine the outcomes of trading strategies based on news sentiment including a 10 bps trading cost from August 2021 to July 2023. Fig. 1 illustrates the performance of various strategies, notably highlighting the long-short OPT strategy with an impressive 355% gain. This underscores the powerful predictive capability of advanced language models in forecasting market movements. Other strategies, such as long-short BERT and long-short FinBERT, also register significant gains of 235% and 165%, in stark contrast to traditional market portfolios, which barely exceed 1%. Conversely, the Loughran-McDonald dictionary model, extensively employed in finance research, managed only a 0.91% return. This pronounced disparity suggests that dictionary-based models do not effectively interpret the nuanced sentiments present in contemporary financial news as efficiently as more advanced language models. This analysis substantiates the importance of employing sophisticated language models in developing investment strategies based on news sentiment.''', '''Result and discussion Data that has been successfully collected and cleaned as well as organized based on the mentioned steps from the Twitter is analyzed further by histogram analysis, word cloud and commonality analysis, cluster dendrogram analysis, as well as pyramid analysis. Each analysis will be detailed in the next section. Fig. 4 Cleaned and organized text examples. This is to give an example of the cleaned and organized data for the next process Subroto and Apriyana J Big Data (2019) 6:50 Page 10 of 19 Feature extraction result and data analysis Te purpose of this stage is to extract and analyze individual words as well as determine the number of occurrences in the corpus. To perform the word analysis, the corpus data type was converted into a term-document matrix (TDM) or a document-term matrix (DTM). Tis matrix includes a document identifcation number as a column and terms as a line, with the matrix element as the frequency of terms [41]. Cyber risk analysis was performed by using the TDM data type with bigram syllables (two syllables in one term). Moreover, the following approaches were applied: Histogram analysis In this case, the highest frequency bigram in the Twitter conversations indicated that the cyber risk was occurring through apache, apache struts, Yahoo, and Cisco, while the attack methods were using the interactive shell, strutsshell interactive, kitploitstrutsshell, and strutspwn exploit. Based on the fndings, Apache was the most trending topic among the Twitter users, see the following Fig. 5. Word cloud and commonality analyses Te word cloud analysis reinforced the results of the histogram analysis; that is, apache struts were the highest frequency bigram, compared to the other related bigrams that discussed apache and struts. Te commonality analysis validated that the keywords used in the Twitter and CVE data collection process were appropriate since there were discussions about vulnerabilities in both platforms as it can be seen on the following Fig. 6. Cluster dendrogram analysis Based on the cluster dendrogram analysis, the terms apache, struts, shell command, interactive, and exploit were found in adjacent clusters. Tis fnding validates that the cyber risk attack on apache struts was using the interactive shell command exploits, see Fig. 7. Pyramid analysis Based on this analysis, apache was the third most frequent unigram in Twitter, compared to the CVE database, thus validating that Apache had the highest cyber risk frequency (Fig. 8). Data training and testing by using SML Te prediction model can be implemented by labeling all the Twitter cyber risk data. In this regard, the threats in the form of exploits can be labeled as CVEs, while the rest can be labeled as NOTCVEs. Te data with the former label indicates that the Twitter status consists of cyber risk occurrences on the CVE website, after which the algorithm will “learn” from the labeled data. In the implementation phase, the model can Fig. 5 Twitter N-gram histogram analysis. This is to show that what people speak mostly on Twitter regarding the cyber-attack. It shows cyber-attack occurred mostly through Apache Subroto and Apriyana J Big Data (2019) 6:50 Page 11 of 19 also determine whether the Twitter status is a cyber risk event. Te algorithms used for such predictions are as follows: • Naive Bayes Tis classifcation is constructed by processing the training data and estimating the probability of each data set, based on the document feature value. • K-nearest neighbors For each data set, the nearest Euclidean distance is determined. • Support vector machines Tis classifcation is determined by generating diferent data separators that have been optimally calculated by Euclidean distance. Fig. 6 Word cloud, comparison dan commonality analysis results. it shows the result of word cloud and commonality where Apache, Exploit, and Vulnerability is very visible among other words twitted during the period Fig. 7 Cluster dendrogram analysis results. This fgure shows the result from the cluster dendrogram where the terms apache, struts, shell command, interactive, and exploit were found in adjacent clusters. Thus, validates that the cyber risk attack on apache struts was using the interactive shell command exploits Subroto and Apriyana J Big Data (2019) 6:50 Page 12 of 19 • Decision trees Tis tool separates the distance between features repeatedly until the overlapped areas disappear. • Artifcial neural networks Neural networks in which the connections between the units do not form a cycle or loop. Te following Fig. 9 is an example of how the model was executed by using artifcial neural networks and R software. Executing Fig. 9 can be done with the following Pseudo Code which runs the data that split into two periods: data training period and data testing period into the model using an artifcial neural network (ANN). Accuracy testing and model selection Accuracy testing was performed by utilizing the following confusion matrix (Fig. 10). In this research, the confusion matrix was used to calculate the accuracy of the predicted results and the actual data proportion. Also, it is important to note the following: • Accuracy was the primary measurement used for model selection. It was obtained from the formula (TP+TN)/(TP+FP+FN+TN). Fig. 8 Pyramid analysis results. This fgure shows, once more, that apache and exploit become the two most twitted and then followed by a vulnerability in the CVE database Subroto and Apriyana J Big Data (2019) 6:50 Page 13 of 19 • Sensitivity was another term for Recall or the hit rate, which presents the true positive ratios. Tis was obtained from the formula TP// (TP+FN). • Positives prediction value was another term for Precision, which presents the ratio of the true positives to the total positives. It was obtained from the formula TP// (TP+FP). Based on these algorithms, the following accuracy measurement was generated (Table 1). Overall, the selected model obtained an accuracy rate of 96.73%. Most of the other models were also relatively accurate (approximately 95%), with the one exception: Te Naive Bayes (e1071) model (accuracy rate of 55%). The practical implication of the model Te anatomy of cyber risk occurrence (Fig. 11) can be identifed from the following software life cycle: Fig. 9 Artifcial neural network model result. This fgure shows the result in term of its accuracy to predict the vulnerability from Twitter that will be posted in the CVE database Fig. 10 Confusion matrix illustration. This fgure illustrates model accuracy by comparing the prediction resulting from the model to the actual condition Subroto and Apriyana J Big Data (2019) 6:50 Page 14 of 19 1. A software product is fnalized, released to the market, and used by consumers. 2. Software vulnerability is found by entities and providers. 3. Te provider validates and verifes the existence of the vulnerability. 4. Once verifed, the provider informs the consumers and the global CVE vulnerability database manager. Te CVE database includes a standardized process of validating information through the stages of declaration, evaluation, and publication. At this point, there is a time lag in which the vulnerability is exposed, but no patch is created. 5. After notifcation, the provider develops the software vulnerability patch. 6. Patch testing is performed to ensure that the vulnerability has been fxed. 7. Te patch is documented to make the consumers understand its function. 8. Te patch is released to the market. However, since the average time lag from notifcation to patch distribution is 100 to 120 days [42], additional vulnerabilities can be exposed. Table 1 Model accuracy comparison and selection Model Accuracy Precision Recall Naïve Bayes [e1071] 0.5531 1.0000 0.4736 Naïve Bayes [Rweka] 0.9449 0.9391 1.0000 Super vector machine 0.9408 0.9807 0.9706 K-nearest neighbor 0.9633 0.9760 0.9807 Decision tree [Ctree] 0.9408 0.9469 0.9856 Decision tree [J48] 0.9571 0.9669 0.9832 Decision tree [C50] 0.9571 0.9669 0.9832 Artifcial neural network 0.9673 0.9762 0.9856 Fig. 11 Cyber risk occurrence illustration. This fgure illustrates how the Cyber Risk can occur and where the stage that this research is useful mostly to mitigate the risk’s occurrence impact Subroto and Apriyana J Big Data (2019) 6:50 Page 15 of 19 9. Te software provider provides patch implementation support services to the consumers. In this process, the following points require serious attention: • Te frst point is between Stages 1 and 2, where there is a potential risk that black-hat hackers can discover a vulnerability and exploit it to attack certain users. In this case, there are only two types of users, i.e., targeted users and non-targeted users, both of which are open to potential cyber risk events without an efective mitigation strategy (other than insurance). • Te second point is regarding the notifcation process in Stage 4, where vulnerability information for the consumers is also received by the black-hat hackers. For these hackers, such information will increase the potential of additional attacks, since the vulnerabilities have already been revealed by the trusted parties. As stated earlier, there is a time lag in this process. According to Ablon, it takes an average of 22 days from when the vulnerability is published until the exploits are created [43]. On the other hand, the provider will need (on average) 100 to 120 days to develop an efective patch after the vulnerability has been published. Te threat at this point is referred to as a “zero-day exploit,” since there is no patch for the vulnerability. Approximately 94% of exploits are created after notifcation, and roughly 5% are real exploits [44]. In this case, there are only three types of users: users that immediately implement a mitigation strategy after notifcation; non-mitigating users; and nontargeted users that do not implement a mitigation strategy. • Te third point is the period after a patch has been distributed. In this regard, a release of a patch can also increase the potential of additional attacks, since blackhat hackers can reverse-engineer the patch code. According to Farmer, it takes (on average) only 9 days for black-hat hackers to create exploits by reverse-engineering a patch code [45]. Finally, the following Fig.  12 is one scenario in which the predictive model can be efectively used by insurance companies that provide cyber risk products. Say that there has been a cyber-attack by a black-hat hacker using “apache struts” vulnerability exploits and successfully penetrate the “X” bank information security system. Tis “X” bank is not a cyber insurance policyholder. Te security breach event is then known by internal parties (information security functions, management) and also by external parties (vendors, customers). Tis “apache struts” exploits information is then reported to CVE for others to take precautions, but the verifcation and administration process takes some times to appear on the CVE web site immediately. Te same exploits information is also written on social media Twitter status and even becoming viral information. Te predictive model will capture and analyze this conversation to then become an automated report for insurance companies that there has been a cyber risk occurrence of “apache struts” vulnerability. Insurance companies will then provide preventive reports (early warning) to all policyholders and recommend to immediately prevent the occurrence of potential cyber-attacks for the same risk patterns. Prevention may include vulnerability patching, tight monitoring or even temporary closing of services if there Subroto and Apriyana J Big Data (2019) 6:50 Page 16 of 19 is no alternative prevention mechanism. Because of the prevention efort for cyber risk occurrence, then the insurance company has reduced the potential spread of risk incident claims. Te same scenario can be applied if the afected cyber-attack company is one of the policyholders; the spread of more claims can be prevented with this preventive mechanism (Fig. 13). To apply the scenario, value chain activity changes are required in the insurance companies. Changes in organization require an additional function to manage and prevent the risk occurrence. Tis function has objectives to minimize the potential spread of risk occurrence and claim. Below are suggested value chain changes described in the Fig. 12 Predictive model implementation scenario illustration. This fgure tells how the result from this research can be useful for the related company, i.e., the insurance company to supply an early warning to their clients Fig. 13 Changes in general insurance company value chain. This fgure illustrates the opportunity that might be some new value-added service that benefcial for both the insurance company and its customers Subroto and Apriyana J Big Data (2019) 6:50 Page 17 of 19 value-added services section, along with the potential for stream revenue generation through new services over the long term''', '''Results Descriptive Statistics and Correlations All analyses were conducted using raw scores (see Table 2). Before analyses were conducted, we examined the distributions of all measures to check for departures from normality using graphical methods and skewness and kurtosis statistics. Based on the range of the skewness and kurtosis, which all fall within the ±2 recommendation (Field, 2009), as well as histograms, no severe departures from normality were found among our measures. To adjust for slight nonnormality of the data, all analyses that were conducted used maximum likelihood estimation with Figure 1. Taxonomy of models for Approach 1. S-CELFP2WCE = CELF Preschool-2 Spanish Word Classes: Expressive; S-CELFP2WCR = CELF Preschool-2 Spanish Word Classes: Receptive; B-EOWPVT = Expressive One-Word Picture Vocabulary Test: Bilingual; E-PPVT4 = Peabody Picture Vocabulary Test–Fourth Edition; E-CELF4WCE = CELF-4 English Word Classes: Expressive; E-CELF4WCR = CELF-4 English Word Classes: Receptive; E-EVT2 = Expressive Vocabulary Test–Second Edition English; S-SSLICM = SSLIC Spanish Morphology; S-CELFP2WS = CELF Preschool-2 Spanish Word Structure; S-CELFP2RC = CELF Preschool-2 Spanish Recalling Sentences; E-TEGI3RD = TEGI: Third-Person Singular; E-TEGIPT = TEGI: Past Tense; E-TROG = Test for Reception of Grammar; E-CELF4WS = CELF-4 English Word Structure; ECELF4RS = CELF-4 English Recalling Sentences; S-KV = Spanish Knowledge Violations; S-ITBK = Spanish Inference Task: Background Knowledge; S-ITI = Spanish Inference Task: Integration; S-ANCL = Spanish Assessment of Narrative Language: Comprehension; S-CELF4USP = CELF-4 Spanish: Understanding Spoken Paragraphs; E-CELF4USP = CELF-4 English: Understanding Spoken Paragraphs; E-LCM = Listening Comprehension Measure; CELF Preschool-2 Spanish = Clinical Evaluation of Language Fundamentals Preschool–Second Edition–Spanish Edition; CELF-4 = Clinical Evaluation of Language Fundamentals–Fourth Edition; SSLIC = Spanish Screener for Language Impairment in Children; TEGI = Test of Early Grammatical Impairment; L = language; E-L = English language; S-L = Spanish language. 2786 Journal of Speech, Language, and Hearing Research • Vol. 61 • 2779–2795 • November 2018 Downloaded from: https://pubs.asha.org Bibliotheek Der Rijksuniversiteit on 04/28/2024, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions robust standard errors. Table 3 presents the first-order Pearson correlations among all measures, which ranged in size from small to large and were (except for the one correlation noted in bold) statistically significant at p < .05. Approach 1: English- and Spanish-Specific Bifactor Model To build the bifactor model, we first ran two preliminary models: (a) a unidimensional model (Model 1, Table 4) and (b) a two-dimensional Spanish-versus-English model (Model 2, Table 4) where all the Spanish measures loaded into one construct and all the English measures loaded into a second construct. Model fit statistics for confirmatory models are presented in Table 4. For the unidimensional model (Model 1, Table 4), all standardized loadings were significant and ranged in size from 0.34 to 0.83, with only two measures being smaller than 0.40. However, model fit was not acceptable based on the RMSEA (0.158), CFI (0.66), and SRMR (0.13). For the two languagespecific factors (Model 2, Table 4), model fit indices were acceptable based on the RMSEA (0.084), CFI (0.90), and SRMR (0.07). Correlations between the Spanish and English factor were of medium size and significant, r = .55, p < .001. Standardized loadings were all significant, and the magnitudes of the indicators were similar to those of the unidimensional model. The S-B χ2 difference test comparing the nested unidimensional model and the two-dimensional Spanishversus-English model suggested that the two-dimensional model had a significantly better fit than the more parsimonious unidimensional model (S-B χ2 = 26.27, p < .001). Building from the two-dimensional Spanish-versusEnglish model, we then fit a bifactor model by defining a common language factor L and two language-specific Spanish and English factors (Model 3, Table 4). Model fit for the bifactor model was acceptable based on RMSEA (0.063) and excellent based on CFI (0.95) and SRMR (0.05). Standardized loadings for the general factor were all significant and ranged in size from 0.32 to 0.83. As for the Spanishspecific factor, there were four nonsignificant indicators: Figure 2. Taxonomy of models for Approach 2. S-CELFP2WCE = CELF Preschool-2 Spanish Word Classes: Expressive; S-CELFP2WCR = CELF Preschool-2 Spanish Word Classes: Receptive; B-EOWPVT = Expressive One-Word Picture Vocabulary Test: Bilingual; E-PPVT4 = Peabody Picture Vocabulary Test–Fourth Edition; E-CELF4WCE = CELF-4 English Word Classes: Expressive; E-CELF4WCR = CELF-4 English Word Classes: Receptive; E-EVT2 = Expressive Vocabulary Test–Second Edition English; S-SSLICM = SSLIC Spanish Morphology; S-CELFP2WS = CELF Preschool-2 Spanish Word Structure; S-CELFP2RC = CELF Preschool-2 Spanish Recalling Sentences; E-TEGI3RD = TEGI: Third-Person Singular; E-TEGIPT = TEGI: Past Tense; E-TROG = Test for Reception of Grammar; E-CELF4WS = CELF-4 English Word Structure; ECELF4RS = CELF-4 English Recalling Sentences; S-KV = Spanish Knowledge Violations; S-ITBK = Spanish Inference Task: Background Knowledge; S-ITI = Spanish Inference Task: Integration; S-ANCL = Spanish Assessment of Narrative Language: Comprehension; S-CELF4USP = CELF-4 Spanish: Understanding Spoken Paragraphs; E-CELF4USP = CELF-4 English: Understanding Spoken Paragraphs; E-LCM = Listening Comprehension Measure; CELF Preschool-2 Spanish = Clinical Evaluation of Language Fundamentals Preschool–Second Edition–Spanish Edition; CELF-4 = Clinical Evaluation of Language Fundamentals–Fourth Edition; SSLIC = Spanish Screener for Language Impairment in Children; TEGI = Test of Early Grammatical Impairment; V = vocabulary; G = grammar; HLL = higher level language; S = Spanish; E = English. Gray et al.: Dimensionality of Language in Kindergarten 2787 Downloaded from: https://pubs.asha.org Bibliotheek Der Rijksuniversiteit on 04/28/2024, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions Table 3. Bivariate correlations among oral language measures. Measures 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 1. B-EOWPVT 1.00 — 2. S-CELFP2WCE .38 1.00 — 3. S-CELFP2WCR .20 .48 1.00 — 4. E-CELF4WCE .44 .49 .32 1.00 — 5. E-CELF4WCR .19 .15 .31 .63 1.00 — 6. E-PPVT4 .52 .38 .19 .60 .34 1.00 — 7. E-EVT2 .49 .28 .13 .59 .28 .79 1.00 — 8. S-SSLICM .28 .40 .26 .26 .21 .16 .13 1.00 — 9. S-CELFP2WS .31 .42 .30 .30 .13 .15 .14 .76 1.00 — 10. S-CELFP2RS .30 .45 .29 .33 .13 .19 .17 .74 .80 1.00 — 11. E-TEGIRD .46 .29 .19 .58 .35 .55 .63 .28 .33 .33 1.00 — 12. E-TEGIPT .43 .35 .20 .49 .21 .56 .57 .15 .29 .31 .52 1.00 — 13. E-TROG .46 .36 .19 .58 .31 .61 .61 .16 .17 .19 .47 .52 1.00 — 14. E-CELF4WS .51 .31 .16 .62 .31 .66 .69 .30 .33 .32 .73 .58 .59 1.00 — 15. E-CELF4RS .56 .39 .19 .62 .27 .66 .71 .30 .37 .45 .68 .64 .60 .72 1.00 — 16. S-KV .36 .38 .28 .42 .25 .36 .31 .56 .56 .56 .38 .41 .29 .38 .40 1.00 — 17. S-ITI .42 .42 .25 .41 .25 .39 .29 .58 .62 .64 .34 .34 .31 .39 .42 .59 1.00 — 18. S-ITBK .39 .41 .28 .36 .21 .34 .24 .56 .59 .59 .29 .29 .30 .38 .37 .56 .75 1.00 — 19. S-ANCL .44 .46 .36 .43 .28 .37 .30 .66 .68 .72 .33 .30 .29 .44 .45 .60 .75 .75 1.00 — 20. S-CELF4USP .40 .43 .36 .36 .22 .37 .29 .56 .54 .61 .32 .32 .30 .37 .39 .53 .61 .65 .71 1.00 — 21. E-CELF4USP .36 .31 .22 .50 .28 .55 .56 .28 .33 .34 .49 .41 .46 .55 .59 .37 .40 .38 .42 .41 1.00 — 22. E-LCM .50 .34 .12 .52 .25 .64 .65 .27 .32 .38 .53 .45 .56 .62 .66 .43 .49 .47 .54 .49 .62 1.00 — 23. S-LP −.12 .20 .07 −.01 .05 −.14 −.14 .43 .43 .42 .03 .02 −.09 −.01 .02 .20 .26 .26 .27 .30 .04 −.01 1.00 — 24. E-LP .31 .23 .10 .40 .23 .47 .50 .25 .27 .33 .50 .41 .40 .50 .55 .17 .29 .23 .31 .24 .36 .43 .03 1.00 Note. Bold correlations are nonsignificant; all other correlations are significant at p < .05. B-EOWPVT = Expressive One-Word Picture Vocabulary Test: Spanish-Bilingual Edition; S-CELFP2 = Clinical Evaluation of Language Fundamentals Preschool–Second Edition–Spanish Edition; WCE = Word Classes Expressive; WCR = Word Classes Receptive; WS = Word Structure; RS = Recalling Sentences; E-CELF4 = Clinical Evaluation of Language Fundamentals–Fourth Edition; E-PPVT4 = Peabody Picture Vocabulary Test–Fourth Edition; E-EVT2 = Expressive Vocabulary Test–Second Edition English; S-SSLICM = Spanish Screener for Language Impairment in Children: Morphology; E-TEGI = Test of Early Grammatical Impairment; RD = third-person singular; PT = past tense; E-TROG = Test for Reception of Grammar; S-KV = Spanish Knowledge Violations; S-ITI = Spanish Inference Task: Integration; S-ITBK = Spanish Inference Task: Background Knowledge; S-ANCL = Spanish Assessment of Narrative Language: Comprehension; S-CELF4USP = CELF-4 Spanish: Understanding Spoken Paragraphs; E-CELF4USP = CELF-4 English: Understanding Spoken Paragraphs; E-LCM = English Listening Comprehension Measure; S-LP = Spanish Language Proficiency; E-LP = English Language Proficiency. 2788 Journal of Speech, Language, and Hearing Research • Vol. 61 • 2779–2795 • November 2018 Downloaded from: https://pubs.asha.org Bibliotheek Der Rijksuniversiteit on 04/28/2024, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions Word Classes Receptive (loading = 0.13, p = .17), Inference Task: Background Knowledge (loading = 0.21, p = .24), Inference Task: Integration (loading = 0.27, p = .13), and Understanding Spoken Paragraphs (loading = 0.25, p = .10). For the English-specific factor, all loadings were significant, and magnitudes ranged from 0.21 to 0.75. In summary, in the first approach leading to the English- and Spanish-specific bifactor model, we fit a series of three models: a unidimensional model, a two-dimensional English-versus-Spanish model, and an English- and Spanishspecific bifactor model. Among these three models, model fit evidence suggested that the bifactor model was the one that fitted the data best. However, as mentioned previously, one of the drawbacks of a bifactor model is the orthogonality constraint on the correlations among the factors. Thus, we considered a second approach in which a series of construct specific factors (i.e., vocabulary, grammar, and higher level language) were specified. This approach allowed for interfactor correlations and consisted of four models (i.e., models 4– 7 in Table 4 and Figure 2) that will be described next. Approach 2: Construct Distinct Language Models Based on LARRC (2015a), which suggested that language was unidimensional for PK and K, two-dimensional for G1 and G2, and three dimensional for G3, we tested a series of models (Models 4–7 in Table 4 and Figure 2) that defined language constructs disregarding the language of the construct (i.e., vocabulary, grammar, and higher level language) or defined the constructs by English or Spanish (i.e., English vocabulary, English grammar, English higher level language, and Spanish vocabulary, Spanish grammar, and Spanish higher level language). The results presented in Model 4, Table 4, summarize the model fit for the three-factor model, for which a vocabulary, grammar, and higher level language construct were defined irrespective of language. Standardized loadings for all three factors were significant and ranged from 0.30 to 0.86. Model fit was poor based on RMSEA (0.14), CFI (0.73), and SRMR (0.14). With respect to interfactor correlations, the correlations between higher level language with vocabulary (r = .68) and grammar (r = .71) were large and significant. The correlation between grammar and vocabulary was close to one (r = .97) suggesting that grammar and vocabulary could be defined as one construct. According to Brown (2006), to achieve good discrimination, interfactor correlation is recommended to be < 0.85 or, more conservatively, < 0.80. Model fit for the six-factor model (Model 5, Figure 2) is presented in Table 4. For this model, there was a convergence problem related to a nonpositive definite latent variance–covariance matrix, so results presented in Table 3 are not interpretable. Further examination of the interfactor correlations among the six-factor model (see Table 5) suggested that the nonpositive definite problem was likely Table 4. Results of confirmatory models (n = 259). Model no. Model LL χ2 df p MLR scaling factor RMSEA, p close [90% CI] CFI SRMR AIC Approach 1: English- and Spanish-specific bifactor models 1 Unidimensional: L −6485.29 1524.97 205 < .001 0.93 0.158, p < .001 [.150, .165] 0.66 0.13 13110.58 2 Two factors: S + E −6061.89 578.24 204 < .001 0.98 0.084, p < .001 [.076, .092] 0.90 0.07 12265.78 3 Bifactor model: L + S + E −5957.44 370.98 183 < .001 0.96 0.063, p = .011 [.054, .072] 0.95 0.05 12098.89 Approach 2: construct distinct language models 4 Three factors: V + G + HLL −6382.53 1257.60 202 < .001 0.96 0.142, p = .142 [.135, .150] 0.73 0.14 12911.07 5 Six factorsa : E-V + E-G + E-HLL + S-V + S-G + S-HLL −5950.07 350.79 191 < .001 0.97 0.057, p = .057, [.047, .066] 0.96 0.05 12068.15 6 Five factors: E-VG + E-HLL + S-V + S-G + S-HLL −5970.48 391.83 196 < .001 0.97 0.062, p = .062, [.053, .071] 0.95 0.05 12098.97 7 Four factors: E-VGHLL + S-V + S-G + S-HLL −5991.88 433.50 200 < .001 0.98 0.067, p = .067, [.058, .076] 0.94 0.05 12133.78 Note. LL = log likelihood; MLR = maximum likelihood ratio; RMSEA = root-mean-square error of approximation; CI = confidence interval; CFI = comparative fit index; SRMR = standardized root-mean-square residual; AIC = Akaike’s information criteria; L = language general factor with all indicators; S = Spanish; E = English; V = vocabulary; G = grammar; HLL = higher level language; VG = vocabulary and grammar; VGHLL = vocabulary, grammar, and higher level language. a Noninterpretable solution due to a nonpositive definite latent variance–covariance matrix. Table 5. Interfactor correlations for the six-factor model (Model 5). Variable S-V S-G S-HLL E-V E-G E-HLL S-V 1.00 — S-G .56 1.00 — S-HLL .61 .85 1.00 — E-V .58 .33 .56 1.00 — E-G .48 .45 .54 .99 1.00 — E-HLL .45 .46 .69 .92 .88 1.00 Note. All correlations are significant at p < .001. S = Spanish; E = English; V = vocabulary; G = grammar; HLL = higher level language. Gray et al.: Dimensionality of Language in Kindergarten 2789 Downloaded from: https://pubs.asha.org Bibliotheek Der Rijksuniversiteit on 04/28/2024, Terms of Use: https://pubs.asha.org/pubs/rights_and_permissions related to the linear dependency among the grammar English and vocabulary English constructs, for which the correlation was close to perfect (r = .99). To address this dependency, grammar English and vocabulary English were combined into a single construct, and the model was rerun as a five-factor structure (Model 6, Table 4). Model fit for this model was good based on RMSEA (0.07), CFI (0.95), and SRMR (0.05). All standardized loadings were significant, and except for the English Word Classes Receptive indicator, which loading was 0.36, all other loadings were greater than 0.57. Table 6 presents the interfactor correlation for this five-factor model, where the magnitude of the correlations ranges from medium to large. A closer look at these correlations revealed a potential problem with the lack of discrimination between the combined vocabulary– grammar English factor (E-VG) and the higher level language English factor (E-HLL; r = .89). Further examination of the average variance explained (AVE) revealed that the E-VG construct was able to explain an average of 52% of the variance in the set of its indicators, with E-HLL explaining an average of 62% of the variance in its set of indicators. These AVEs were substantially lower than the variance shared between the constructs of E-VG and E-HLL, which was 79%. Paired with the high correlation between these two constructs, these results give support to combine both the E-VG and the E-HLL. In addition, there is precedent of a unidimensional language factor in the study of monolingual English children enrolled in K (LARRC, 2015b). Thus, a four-factor model (Model 7) was specified where all English measures were combined into one construct, and the Spanish constructs remained as three separate factors. Results of the four-factor model (Model 7, Figure 2) are presented in Table 4. Overall, model fit was good as judged by RMSEA (0.067), CFI (0.94), and SRMR (0.05). Standardized loadings for all factors were all significant and with magnitudes comparable to the previous models. The S-B χ2 difference test comparing the nested four-factor model and the five-factor model suggested that the fivefactor model was a significantly better fit than the more parsimonious four-factor language-specific model (S-B χ2 = 49.69, p < .001). However, as discussed previously, the lack of discrimination between the E-VG and E-HLL and evidence of a unidimensional English language construct in K made the five-factor model a poor choice. Thus, we chose to present the four-factor model as the best emerging model from the second approach. Interfactor correlations for the four-factor languagespecific CFA are reported in Table 7. These interfactor correlations revealed a high correlation (r = .85) between two Spanish constructs: higher level language Spanish (S-HLL) and grammar Spanish (S-G). Based on the borderline high correlation between S-HLL and S-G, there is a potential concern for poor discrimination between these two factors. To appease this concern, a model in which these two constructs were collapsed into one was run. In other words, all English measures loaded into one factor, S-HLL and S-G loaded into a second factor, and Spanish vocabulary measures loaded into a third factor were run. Results of this model are not reported in Table 4. Overall, fit indices were no better than those of the four-factor model (i.e., RMSEA = 0.083, CFA = 0.91, SRMR = 0.062). Further, the AVEs were also examined. The S-HLL construct was able to explain an average of 66% of the variance in its set of indicators, with S-G explaining an average of 78% of the variance in its set of indicators. The variance shared between these two constructs, 72%, was slightly higher for the S-G construct but not for the S-HLL construct. Given the conflicting evidence from the AVEs, the borderline correlation between the two constructs, and support from the S-B χ2 difference test in favor of the four-factor model (S-B χ2 = 60.26, p < .001), our decision to present the four-factor model as the best model from the second approach still remains. In summary, results based on a comprehensive look at complexity and desire for parsimony, model fit indices, factor discrimination, and previous empirical evidence of the unidimensionality of English, point at the four-factor model (Model 7, Table 4) as the overall best model from the second approach. Although results did not provide an overwhelming support for a given model among the set we tested (i.e., some of the fit indices were equally acceptable across models), the four-factor model that we present provides information about the interfactor correlation between the English and Spanish constructs and is consistent with previous empirical evidence.''', '''Results The results of both previously mentioned experiments are presented in Tables 2 and 3. Table 2. Results for experiment 1 system diagnosis medical diagnosis diagnosed persons diagnosis results TP FP TN FN colorblind group protanopia 1 1 0 0 0 protanopia dichromacy 4 4 0 0 0 anomalous protanomaly 1 1 0 0 0 trichromacy deuteranopia dichromacy 4 4 0 0 0 anomalous tritanomaly 2 2 0 0 0 trichromacy summary 12 12 0 0 0 control group proper color vision proper color vision 7 7 0 0 0 protanomaly proper color vision 2 0 2 0 0 deuteranopia proper color vision 1 0 1 0 0 deuteranomaly proper color vision 1 0 1 0 0 tritanomaly proper color vision 1 0 1 0 0 summary 12 7 5 0 0 Table 3. Results for experiment 2 system diagnosis medical diagnosis diagnosed persons number of TP FP TN FN colorblind group protanopia 1 1 0 0 0 protanopia dichromacy 4 4 0 0 0 anomalous protanomaly 1 1 0 0 0 trichromacy deuteranopia dichromacy 4 4 0 0 0 anomalous tritanomaly 2 2 0 0 0 trichromacy summary 12 12 0 0 0 control group proper color vision proper color vision 9 9 0 0 0 protanomaly proper color vision 1 0 1 0 0 deuteranopia proper color vision 1 0 1 0 0 tritanomaly proper color vision 1 0 1 0 0 summary 12 9 3 0 0 157 Maciej Laskowski Research Analysis and Conclusions The analysis of all the experiments based on the proposed interactive method for detecting colorblindness show that it is an interesting alternative to currently used methods, especially considering screening tests. The first experiment proved that individual features, such as agility or reflex, have a negative influence on the diagnostic process, as – due to a higher number of errors – the system classified almost 2/3 more interviewees as potentially colorblind (in comparison to the second experiment). However, it must be noted that all confirmed cases of colorblindness were detected and diagnosed properly. Each “false positive” was later excluded by expert evaluation. On the other hand, switching from a 2D to 3D environment had almost no impact on the diagnostic process – the results of the Colorblind Maze experiment neared the results of the “select a square” game. The amount of time needed for developing a 3D game and the computing power needed to run it, however, disputes the necessity and profitability of such a solution. Moreover, the game-based method is designed to be used mainly for screening tests – making quick diagnosis a priority. Considering this, reserving more than 5 minutes per tested person for completion of the game is simply unacceptable. Although it was proven that all of the proposed games may be used for colorblindness detection, the most optimal game – in terms of speed, simplicity and relative low random error rate – turned out to be the simplest one, “select a square”. This is actually an advantage, as its technical requirements are low and the rules are simple – features which should allow it to reach a wider audience.''']\n"
      ],
      "metadata": {
        "id": "ou1m1ZpV31yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries_CGPT = ['''The evaluation of the model's response to different encoding levels reveals an increase in Rouge Scores up to three levels, followed by a decline thereafter. The computational costs, measured in training time per epoch on the Gigaword dataset, show an average increase of about one hour per epoch with each successive encoding level, likely due to weight-sharing across levels.\n",
        "On the Gigaword Test Corpus-1951, the model performs significantly better at three encoding levels compared to lower and higher levels, with Rouge Scores peaking at three levels. It outperforms several baseline models in Rouge-1, Rouge-2, and Rouge-L F1 Scores.\n",
        "Similar trends are observed on the DUC2004 Task1 Corpus, with the model's performance surpassing multiple baseline models, particularly at three encoding levels. The Rouge Scores on the internal Gigaword Test Corpus-10,000 also demonstrate the model's effectiveness, with high recall and precision percentages.\n",
        "Human evaluation supports the model's efficacy in enhancing readability and grammaticality with multiple encoding levels, as assessed by a group of human annotators. Cohen’s Kappa reliability statistic indicates moderate to strong agreement among the annotators, further validating the results.\n",
        "''', '''The evaluation section reports on a study of 1,822 patients with unilateral tibial shaft fractures (TSFs) treated operatively, where 45% developed infections. Feature selection identified seven relevant variables for algorithm development, including classification, bone loss, and age. Five machine learning algorithms showed discriminative performance, with AUC ranging from 0.67 to 0.75 in the derivation cohort. The penalized logistic regression, Bayes point machine, and boosted decision tree models outperformed others. In the validation cohort, AUC ranged from 0.80 to 0.82. The penalized logistic regression model was chosen as the final model due to better calibration and similar performance. This model incorporated predictors such as fracture type and mechanism of injury. An online prediction tool was developed based on this model, allowing users to calculate infection probabilities postoperative treatment of TSFs.''', '''The evaluation focused on semantic relatedness, considering relationships between words like \"is-a,\" \"part-of,\" and \"contains\" defined by global ontologies like WordNet, SUMO, and ConceptNet. Accuracy was measured using correlation values between human judgment and similarity computed by the proposed method, with higher correlation indicating higher accuracy. In the Solar domain, similarity between concept pairs such as (Planet & Earth), (Earth & Sun) was manually computed and compared with human similarity scores. Pearson correlation was used to calculate accuracy, resulting in 0.863 in the Solar domain. The proposed method's accuracy was compared with other measures using standard datasets, showing superior performance across various measures, as depicted in Table 6.''', '''The experimental evaluation section begins with an overview of the experimental setup and datasets used. An 8-node parallel cluster was employed, each node featuring a Pentium Quadcore CPU, 8 GB RAM, and 1 TB disk space. R and C++ were utilized for solution development, with data sets split and transferred among processing nodes using standard UNIX commands. Two base data sets, YearPredictionMSD and CreditCard, obtained from the UCI machine learning repository, were sampled and replicated to mimic large data sets.\n",
        "The evaluation further delves into computing descriptive statistics on data subsets, demonstrating scalability and performance using the summarization matrix approach. Time measurements for computing mean comparison on data subsets in parallel are provided, illustrating efficient computation across different data set sizes.\n",
        "Comparison with other parallel systems, including Spark and a parallel columnar DBMS (Vertica), is presented. Time comparisons for computing machine learning models and summarization matrix using different approaches are detailed, with emphasis on the trade-offs between computation in a single machine versus a parallel cluster. It's noted that while parallel processing accelerates computation, a single machine may outperform in scenarios involving smaller data sets, highlighting the importance of understanding trade-offs based on data set size and computational complexity.\n",
        "''', '''The evaluation section of the article provides insights into the trends and challenges in various applications within the domain of software repositories (MSR). The analysis indicates a significant increase in research on summarization and sentiment analysis, with approximately 48 percent of papers focusing on these topics, highlighting their importance in natural language processing (NLP). Mobile analytics saw a substantial increase in research from 2012 to 2015, particularly when NLP techniques were involved. While extractive summarization research reached a saturation stage after 2013, there's ongoing exploration in abstractive summarization. Sentiment analysis research is steadily growing, while norms mining remains relatively underexplored.\n",
        "\n",
        "The discussion extends to future research areas and challenges. Context-based sentiment analysis is identified as a key research gap, with existing tools showing limitations in handling domain-specific technical terms prevalent in software engineering. Norms mining in open-source development communities presents opportunities for understanding the emergence, adoption, and impact of norms, alongside challenges related to cultural differences and noisy, incomplete data. Mobile analytics pose challenges in handling massive, unstructured user review data, including issues with abbreviations and informal language.\n",
        "\n",
        "Regarding summarization techniques, questions remain open about what constitutes a good summary and how to evaluate it effectively. Abstractive summarization, although promising, faces challenges due to the complexity of NLP, including pronoun resolution and semantic representation. The evaluation of summaries needs to consider multiple aspects beyond informativeness. Challenges also exist in automatic documentation generation and unit test case summarization, indicating areas for further exploration and improvement.\n",
        "\n",
        "Overall, the discussion highlights the evolving landscape of research in MSR, emphasizing the need for addressing existing gaps and overcoming challenges to advance the field's capabilities and applications.\n",
        "''', '''The results section presents findings from experiments conducted over 5 months across four courses, using data from Blackboard, Facebook, and WhatsApp. A sentiment analysis tool, SALE, is employed, assigning scores to words indicating positive or negative sentiment. Seed words specific to teaching are defined, and their sentiment scores are calculated based on relevance. Measures including knowledge, course content, teaching style, and assessment are analyzed using social media data and manual surveys. The comparison between automated analysis and survey results reveals similar patterns, with notable differences in teaching methodology assessment. Social media analysis reflects more accurate student feedback, highlighting areas for curriculum and teaching improvement. For instance, while students rated Computer Architecture highly for knowledge and understanding, social media analysis indicated ongoing discussion and lower satisfaction. Complaints about exam results were prominent on social media, suggesting opportunities for assessment improvement and transparency enhancement.''', '''The evaluation section presents findings from analyzing sentiment in U.S. financial news using language models (LLMs). The study processed 965,375 articles and evaluated four models: OPT, BERT, FinBERT, and Loughran-McDonald. OPT demonstrated the highest accuracy, followed by BERT and FinBERT, while the traditional Loughran-McDonald model lagged significantly behind. Regression analyses revealed OPT's strong correlation with next-day stock returns, followed by FinBERT and BERT. Factors like model design and training data specificity contribute to these differences. Portfolio management strategies based on sentiment analysis further demonstrated the superior performance of advanced LLMs compared to traditional methods. Long-short OPT strategy outperformed others significantly, indicating the predictive capability of advanced LLMs in forecasting market movements. Overall, the study emphasizes the importance of employing sophisticated language models for accurate sentiment analysis and investment decision-making in financial markets.''', '''The evaluation section presents results and discussions on data analysis techniques applied to Twitter data for cyber risk analysis. Techniques include histogram analysis, word cloud and commonality analysis, cluster dendrogram analysis, and pyramid analysis. The histogram analysis revealed Apache as the most discussed topic, while word cloud and commonality analyses reinforced this finding. Cluster dendrogram analysis confirmed the relationship between terms like Apache, Struts, and shell commands. Pyramid analysis validated Apache's prominence in Twitter discussions. Machine learning models like Naive Bayes, K-nearest neighbors, and artificial neural networks were utilized for prediction with an overall accuracy of 96.73%. Practical implications of the model include identifying cyber risk occurrence stages and suggesting preventive measures. The scenario demonstrates how insurance companies can utilize predictive models for early warnings and risk mitigation strategies, leading to potential value chain changes and revenue generation opportunities.''', '''The evaluation section presents the findings of two approaches in constructing language models for bilingual kindergarteners. Approach 1 focuses on an English- and Spanish-specific bifactor model, revealing that a two-dimensional model with separate language factors fits the data best. However, a bifactor model's drawback prompts a second approach. Approach 2 examines construct-specific language models, revealing that a four-factor model, where all English measures are combined into one factor and Spanish constructs remain separate, emerges as the best model. Interfactor correlations suggest potential concerns regarding discrimination between certain factors, but the four-factor model is deemed the most suitable based on various criteria, including model fit, factor discrimination, and previous empirical evidence.''', '''The evaluation of the experiments on colorblindness detection indicates that the proposed interactive method is promising, particularly for screening tests. Experiment 1 highlighted the impact of individual factors on diagnostics, resulting in more false positives compared to Experiment 2. However, all confirmed colorblind cases were accurately diagnosed, with false positives later dismissed by experts. Experiment 2, utilizing a 3D environment, showed similar diagnostic efficacy to the 2D method, but the resource-intensive nature of 3D development raises questions about its practicality. Despite this, the \"select a square\" game emerged as the most optimal, given its simplicity, speed, and low error rate. Its accessibility and simplicity make it suitable for wider adoption in colorblindness detection.''']\n",
        "\n",
        "\n",
        "summaries_BARD = ['''The study evaluated a model named MED for abstractive summarization. The model's performance was assessed using Rouge Scores on Gigaword and DUC2004 datasets. It was found that increasing the number of encoding levels in the model improved performance up to a certain point (3 levels for Gigaword and DUC2004). Beyond 3 levels, performance decreased. For Gigaword, with 3 encoding levels, MED achieved a Rouge-1 F1 Score of 36.72, significantly outperforming all baseline models. Similar improvements were observed on DUC2004. Additionally, human evaluation corroborated these findings, indicating that summaries generated by MED with 2 or 3 encoding levels were rated higher for grammaticality and readability compared to those generated with a single level.''', '''The study evaluated a machine learning model to predict infections after surgery for tibial shaft fractures (TSFs). Out of 1822 patients, 170 (9%) developed infections.\n",
        "The model identified 7 important factors for predicting infections: fracture classification, bone loss, injury mechanism, multitrauma, another fracture classification system, age and fracture location.\n",
        "Three machine learning algorithms performed well: penalized logistic regression, Bayes point machine, and boosted decision tree. The penalized logistic regression model was chosen as the final model due to better calibration.\n",
        "This model considers factors like fracture type, age, and injury cause to predict infection risk. An online tool based on this model is available to calculate infection probability for individual patients.\n",
        "''', '''The evaluation section assesses a new method for measuring semantic relatedness between words, particularly focused on the domain of solar energy.\n",
        "Accuracy Measure:\n",
        "•\tThe method leverages existing resources (WordNet, SUMO, ConceptNet) to define semantic relationships (\"is-a\", \"part-of\", etc.).\n",
        "•\tA mathematical formula (Eqn. 1, not shown here) calculates the similarity score between word pairs.\n",
        "•\tAccuracy is based on how well the model's similarity scores align with human judgments of relatedness (higher correlation means better accuracy).\n",
        "Results on Solar Domain:\n",
        "•\tHuman experts rated the similarity of various word pairs in the solar domain on a 0-4 scale (e.g., Planet-Sun: 3.6).\n",
        "•\tThe new method achieved a high accuracy (0.863) when compared to these human judgements.\n",
        "Comparison with Existing Methods:\n",
        "•\tThe proposed method's performance is evaluated against established measures on three standard datasets (Miller & Charles, Robenstein & Goodenough, and 353 word similarity).\n",
        "•\tTable 6 (not shown here) demonstrates that the new method outperforms prior methods on all datasets. For instance, it achieves a higher accuracy (0.86) compared to existing approaches like Feature based Approach (0.82) and Semantic Relatedness (0.83).\n",
        "Conclusion:\n",
        "This new method shows promise for measuring semantic relatedness, especially within the solar energy domain. It achieves high accuracy compared to human judgment and outperforms existing methods on standard datasets.\n",
        "''', '''Evaluation of a Scalable Summarization Method for Machine Learning\n",
        "This section evaluates a new method for creating summaries of large datasets (scalable summarization) used in machine learning and statistical analysis. The goal is to speed up computations without sacrificing accuracy.\n",
        "Evaluation Setup:\n",
        "•\tHardware: 8-node parallel cluster with Pentium Quadcore CPUs (1.6 GHz), 8 GB RAM, and 1 TB disk space per node.\n",
        "•\tSoftware: R (data science language) with C++ for the new method, Spark-MLlib library for comparison with Spark (popular big data system), and Vertica (parallel DBMS) for comparison with a parallel database system.\n",
        "•\tDatasets: YearPredictionMSD and CreditCard (public datasets from UCI machine learning repository) were replicated and resized to simulate larger datasets.\n",
        "Evaluation Methods:\n",
        "•\tAccuracy: Measured by how well the new method's similarity scores between words align with human judgments of relatedness (higher correlation signifies better accuracy).\n",
        "•\tTime Comparison: The new method is compared to Spark and Vertica in terms of time taken to compute:\n",
        "o\tDescriptive statistics and statistical tests on data subsets.\n",
        "o\tMachine learning models (Linear Regression, PCA, Naive Bayes, K-means) on varying data sizes.\n",
        "•\tTrade-offs: Evaluated the balance between using a single machine vs a parallel cluster for computing the summarization matrix depending on data size.\n",
        "Key Findings:\n",
        "•\tThe new method achieves high accuracy in measuring semantic relatedness (compared to human judgements).\n",
        "•\tThe new method outperforms Spark for all machine learning models on all datasets, especially when data needs to be partitioned first.\n",
        "•\tThe new method is faster than Vertica (parallel DBMS) for computing the summarization matrix, particularly when data partitioning is not required.\n",
        "•\tUsing a single machine is faster for small datasets (less than 1 million data points and 10 dimensions) due to overhead in parallel processing.\n",
        "•\tParallel clusters become significantly faster (at least 2x) for large datasets (over 1 million data points) due to limitations of a single machine's memory.\n",
        "Limitations of the New Method:\n",
        "•\tMay not work well with distributions that cannot be summarized by the Gaussian distribution (e.g., exponential distribution).\n",
        "•\tOriginal data cannot be retrieved from the summarization matrix.\n",
        "Conclusion:\n",
        "This new scalable summarization method is a promising approach for speeding up machine learning and statistical analysis on large datasets, especially when compared to existing solutions like Spark and Vertica. The choice between a single machine and a parallel cluster depends on data size.\n",
        "''', '''This evaluation analyzed research trends in Natural Language Processing (NLP) applied to software engineering tasks. Here's a summary of the key findings:\n",
        "•\tMost Studied Topics: Sentiment analysis and summarization are the most researched areas in NLP for software engineering, with almost half (48%) of the 257 reviewed papers focusing on them. There's a growing interest in mobile analytics (research from 2012 to 2015).\n",
        "•\tResearch Phases: Research in extractive summarization has matured, with a shift towards abstractive summarization techniques. Sentiment analysis research is steadily growing. Norms mining, a field that analyzes how developers interact and establish norms, is a new area with just 16 papers published so far.\n",
        "•\tChallenges:\n",
        "o\tSentiment analysis: Current sentiment analysis tools perform poorly on software engineering data due to technical terms and informal language. There's a need for context-aware sentiment analysis and better handling of sarcasm and irony.\n",
        "o\tNorms mining: More research is needed to understand how norms differ in projects, how norms relate to developers' roles, and the impact of cultural differences.\n",
        "o\tMobile analytics: Extracting useful information from massive and unstructured mobile app review data is challenging due to abbreviations, slang, and typos.\n",
        "o\tSummarization: Key challenges include defining \"good\" summaries, proper evaluation metrics, and abstractive summarization techniques (which are more complex than extractive techniques).\n",
        "The future of NLP in software engineering research requires solutions for these challenges and holds promise in areas like automatic documentation generation and unit test case creation.\n",
        "''', '''The researchers conducted sentiment analysis on social media data (Blackboard, Facebook, WhatsApp) and compared it with surveys to gauge student feedback on four areas: knowledge & understanding (K&U), course content, teaching style, and assessment. Here's a summary of the findings:\n",
        "•\tOverall Sentiment: Social media analysis showed a generally positive leaning compared to surveys. This is likely because students feel more comfortable expressing criticism anonymously on social media.\n",
        "•\tAgreement Between Methods: Findings from social media analysis and surveys agreed on the ranking of student satisfaction across the four areas (K&U highest, teaching style lowest). However, there were magnitude differences in scores.\n",
        "•\tKnowledge & Understanding (K&U): Both methods showed a K&U satisfaction of around 70%. However, social media analysis captured a trend of understanding increasing over the semester, which surveys couldn't due to their one-time nature.\n",
        "•\tAssessment: Social media analysis indicated lower satisfaction with assessments (65%) compared to surveys (70%). This suggests students might be unhappy with exam grades and express frustration online.\n",
        "The study suggests that social media sentiment analysis can be a valuable tool to gather student feedback alongside traditional surveys. It can capture ongoing discussions and student frustrations that surveys might miss.\n",
        "''', '''The study assessed the effectiveness of sentiment analysis from financial news articles to predict stock returns. Here's a summary of the key findings:\n",
        "•\tSentiment Analysis Accuracy: Large Language Models (LLMs) like OPT, BERT and FinBERT significantly outperform traditional dictionary-based methods in sentiment analysis of financial news. OPT showed the most accuracy (74.4%) in predicting stock returns based on news sentiment.\n",
        "•\tPredicting Returns with LLMs: OPT also demonstrated the strongest correlation between its sentiment scores and next-day stock returns. BERT and FinBERT also showed promise, but to a lesser extent. The superiority of OPT is likely due to its larger parameter space and advanced training methods.\n",
        "•\tSentiment-Based Portfolio Performance: The long-short portfolio strategy based on OPT's sentiment analysis achieved the highest Sharpe ratio (3.05), indicating strong risk-adjusted returns. This strategy outperformed other sentiment analysis models (BERT, FinBERT) and traditional dictionary-based approaches.\n",
        "•\tExplanation for OPT's Performance: OPT's superior performance is likely due to its ability to capture the nuances of sentiment in financial news compared to simpler models. BERT and FinBERT also showed promise, but potentially suffered from limitations in model design or training data.\n",
        "Overall, the study suggests that sentiment analysis from financial news using advanced LLMs like OPT can be a valuable tool for investment strategies. These models outperform traditional methods in predicting stock returns and can be used to create effective investment portfolios.\n",
        "''', '''Summary of the Evaluation Section in the Scientific Article\n",
        "The researchers analyzed data collected from Twitter conversations about cyber risks. Here's a summary of the key findings:\n",
        "•\tAnalysis Techniques: They used histogram analysis, word cloud analysis, commonality analysis, cluster dendrogram analysis, and pyramid analysis to identify patterns and trends in the Twitter data.\n",
        "•\tKey Findings:\n",
        "o\tThe most frequent cyber attacks discussed on Twitter involved Apache Struts software and exploited interactive shell commands.\n",
        "o\tWords like \"Apache,\" \"Exploit,\" and \"Vulnerability\" were most commonly used in Twitter discussions related to cyber attacks.\n",
        "o\tThe findings aligned with data from the CVE (Common Vulnerabilities and Exposures) database.\n",
        "•\tMachine Learning Model Performance:\n",
        "o\tSeveral machine learning algorithms were used to predict cyber risk events from Twitter data, including Naive Bayes, K-Nearest Neighbors, Support Vector Machines, Decision Trees, and Artificial Neural Networks.\n",
        "o\tThe Artificial Neural Network model achieved the highest accuracy (96.73%) in predicting cyber risk events based on Twitter conversations.\n",
        "o\tOther models also performed well, with accuracy around 95%, except for the Naive Bayes model (e1071) which had a significantly lower accuracy (55%).\n",
        "•\tReal-World Application: The authors propose using the machine learning model to help cyber insurance companies provide early warnings to their clients about potential cyber attacks.\n",
        "o\tThe model would analyze Twitter conversations to identify discussions about vulnerabilities and exploits.\n",
        "o\tInsurance companies could then proactively recommend mitigation strategies to their clients before an attack occurs.\n",
        "Overall, the study demonstrates the potential of using social media data and machine learning to predict cyber risk events and develop early warning systems.\n",
        "''', '''Summary of the Evaluation in the Scientific Article\n",
        "The researchers examined the fit of several statistical models to assess how well they represent the relationships between different measures of oral language skills in kindergarten children. The models included:\n",
        "•\tUnidimensional model: All language skills are explained by a single factor.\n",
        "•\tTwo-dimensional model (Spanish-English): Separate factors explain Spanish and English skills.\n",
        "•\tBifactor model with language and specific skill factors: There's a common underlying language factor and separate factors for specific skills (vocabulary, grammar, higher-level language) in both Spanish and English.\n",
        "•\tThree-factor model (vocabulary, grammar, higher-level language): These factors apply to both Spanish and English skills.\n",
        "•\tSix-factor model: Separate factors for vocabulary, grammar, and higher-level language in both Spanish and English.\n",
        "•\tFive-factor model (combining grammar and vocabulary for English): This addresses near-perfect correlation between English grammar and vocabulary in the six-factor model.\n",
        "•\tFour-factor model (language-specific): One factor for English skills and three separate factors for Spanish vocabulary, grammar, and higher-level language. This combines English grammar and vocabulary due to a lack of clear distinction in the data and aligns with prior research suggesting a single underlying factor for English in kindergarten.\n",
        "•\tThree-factor model (combining Spanish grammar and higher-level language): This explores an alternative to the four-factor model by merging similar Spanish factors.\n",
        "The researchers assessed each model's fit using multiple statistical measures (RMSEA, CFI, SRMR) and considered both complexity (number of factors) and interpretability of the results. Here are the key findings:\n",
        "•\tThe unidimensional and three-factor (all language skills) models provided poor fits to the data.\n",
        "•\tThe six-factor model suffered from factors with near-perfect correlation (English grammar and vocabulary).\n",
        "•\tThe five-factor model addressing this correlation showed potential limitations due to a shared variance between the combined English factor and English higher-level language factor.\n",
        "•\tThe four-factor model (language-specific) emerged as the best solution, balancing model fit with interpretability and considering prior research on English language development in kindergarten.\n",
        "•\tThere was some concern about a high correlation between Spanish grammar and higher-level language factors in the four-factor model, but collapsing them into one factor did not significantly improve the model fit.\n",
        "Overall, the evaluation favored a four-factor model where:\n",
        "•\tEnglish skills are represented by a single factor.\n",
        "•\tSpanish skills are divided into three separate factors for vocabulary, grammar, and higher-level language.\n",
        "''', '''The evaluation assessed two experiments for a new interactive method of detecting colorblindness. Both experiments used a game format: selecting a square in a 2D interface and navigating a maze in 3D.\n",
        "The results showed 100% accuracy in diagnosing confirmed colorblindness for both games. However, the simpler 2D game had a lower error rate in classifying people without colorblindness. Specifically, the 2D game produced about two-thirds fewer false positives compared to the 3D maze.\n",
        "While the 3D maze offered a more immersive experience, it did not significantly improve detection and required more development time and computing power. This makes the simpler 2D game preferable for colorblindness screening tests where speed and accessibility are crucial. Overall, the study suggests the 2D game as a promising alternative for fast and easy colorblindness detection, particularly in screening contexts.\n",
        "''']\n",
        "\n",
        "\n",
        "summaries_YCHT = ['''Summary of Results and Evaluation\n",
        "The evaluation of the model's performance against 6 levels of encoding of the input sequence revealed an ascent in Rouge Scores until the 3rd encoding level, followed by a decline from the 4th level onwards. The computational costs for training the model per epoch on the Gigaword dataset increased with each encoding level, with the finest model taking about 82.56 hours in total to train. The model's training time until convergence ranged between 3 to 4 days, showing a stunted rise in training time with successive increase in encoding levels due to weight-sharing.\n",
        "Rouge Results\n",
        "The Rouge Scores on the Gigaword Test Corpus demonstrated that the model performed significantly better against three encoding levels compared to two levels, but encountered a sharp downturn in accuracy for four, five, and six levels. The model outperformed all baseline models in terms of Rouge F1 Score after three levels of encoding of the input sequence. Similar trends were observed in the Rouge Scores on the DUC2004 Task1 Corpus and the internal Gigaword Test Corpus.\n",
        "Human Evaluation\n",
        "Substantiating claims of enhanced readability and grammaticality using multiple levels of encoding, the model's summaries were assessed by a group of eight human annotators. The summaries with 2 and 3 levels of encoding were rated higher in terms of grammaticality and readability compared to a single level. The inter-rater reliability was substantiated by computing Cohen’s Kappa statistic, indicating moderate to strong agreements between the raters.\n",
        "Overall, the evaluation highlighted the trade-off between Rouge Scores and computational costs with increasing encoding levels, while also emphasizing the model's performance gains in terms of readability and grammaticality with multiple levels of encoding.\n",
        "The original text has been summarized within the specified length limit. If you need further details on any specific aspect, feel free to ask!\n",
        "''', '''Summary of Scientific Article Evaluation Section\n",
        "The evaluation section of the scientific article on tibial shaft fractures (TSFs) discusses the results of the experiments and their implications. The study included 1,822 patients with unilateral TSFs, with a median age of 38 years and 75% being men. 45% of the patients had a closed fracture. Feature selection using random forest algorithms identified 7 relevant variables for algorithm development, with discriminative performance of machine learning (ML) prediction models quantified by the AUC ranging from 0.67 to 0.75 in the derivation cohort. The best-performing ML models were further evaluated in the validation cohort, with discriminative performance quantified by the AUC ranging from 0.80 to 0.82. Based on calibration and performance, the penalized logistic regression-derived prediction model was chosen as the final model. This model was incorporated into an online open-access multiplatform prediction tool. The final model identified several predictors of infection, including Gustilo-Anderson type IIIA and type IIIB, age, AO/OTA type 42C3, a crush injury, and a fall. The results were made available through an online prediction tool for users to calculate the probability of infection in patients after operative treatment of TSFs.\n",
        "The study's findings demonstrate the potential of ML algorithms in predicting subsequent surgery in TSF patients, with the penalized logistic regression model identified as the superior choice for predicting infection after surgery. The study's results have practical implications, as the final model has been made accessible through an online prediction tool for use by healthcare professionals.\n",
        "Overall, the evaluation section provides a detailed overview of the study's results, the performance of ML prediction models, and the practical implications of the findings for predicting infection in patients with TSFs after surgery.\n",
        "''', '''Summary of Evaluation Section in Scientific Article\n",
        "The evaluation section of the scientific article discusses the results gained after experiments and their implications. In the semantic relatedness evaluation, relationships between words defined by global ontologies like WordNet, SUMO, and ConceptNet are considered. The accuracy is computed as the correlation value of human judgment and similarity computed by the proposed method. The results are computed for terms in the solar domain, and various concept pairs in the solar domain are identified and manually computed for similarity. The accuracy in the solar domain is found to be 0.863. Additionally, the results are compared with other measures in three standard datasets, and it is found that the proposed method results are better than previously defined measures. The comparison is shown in Table 6, where the proposed method shows higher accuracy compared to other measures in the three databases\n",
        "1\n",
        ".\n",
        "The evaluation section also discusses the accuracy calculation by finding the Pearson Correlation between human similarity and the proposed method similarity. The accuracy is reported to be 0.863 in the solar domain. Furthermore, the results are compared with other measures in three standard datasets, and it is found that the proposed method results are better than previously defined measures. The comparison is shown in Table 6, where the proposed method shows higher accuracy compared to other measures in the three databases\n",
        "1\n",
        ".\n",
        "In conclusion, the evaluation section of the scientific article presents the results of the semantic relatedness evaluation, demonstrating the accuracy of the proposed method in the solar domain and its superiority over previously defined measures in standard datasets\n",
        "1\n",
        ".\n",
        "''', '''The evaluation section of the scientific article in the domain of computer science presents insightful findings. It reveals that the solution for computing 𝛤 using 𝑁 machines outperforms parallel DBMS solutions, especially as the value of 𝑛 increases. Additionally, it highlights the limitations of DBMS solutions using UDFs, noting their lack of portability and substantial memory requirements for scaling up. The trade-offs between parallel clusters and single machines are examined, demonstrating that parallel processing accelerates computation but may introduce overhead, especially for smaller data sets. The comparison between a single machine and parallel machines for computing 𝛤 and 𝛤 𝑘 shows that a single machine performs better when 𝑛 and 𝑑 are low, while the parallel cluster becomes faster as 𝑛 increases. It emphasizes that the time measurements are specifically for partitioning the data set and computing the summarization matrix, excluding the time required for computing machine learning or statistical models. These results provide valuable insights for optimizing the performance of computation in parallel and single machine environments.''', '''Summary of Computer Science Evaluation Section\n",
        "The evaluation section of the scientific article in computer science discusses the results and implications of various studies on natural language processing (NLP) techniques applied to different artifacts and applications. The studies cover a range of topics such as bug reports processing, source code summarization, sentiment analysis of mobile app reviews, and more.\n",
        "Key Points:\n",
        "•\tThe studies evaluated the application of NLP techniques to bug repositories processing, including duplicate bug reports detection and emotions identification in bug reports.\n",
        "•\tSource code summarization techniques were explored, focusing on languages such as C?? and Java. However, there is a need for more work on summarization for other object-oriented languages.\n",
        "•\tCrowdSourcing was highlighted as an emerging method for collecting information for summarization, with potential for extended use in the field of software summarization.\n",
        "•\tThe evaluation emphasized the need for more research on personalized summaries based on individuals' roles, as well as on generating high-quality summaries for automatic documentation tools.\n",
        "•\tFurther research is required on creating unit test case summaries and applying unit test case summarization to test case generation tools.\n",
        "Overall, the evaluation section emphasizes the need for continued research and development in various areas of NLP techniques and their applications in computer science.\n",
        "This summary captures the key points while keeping the length within the specified limit.\n",
        "''', '''Summary of Results Section:\n",
        "The experiments involved data collected from Blackboard, Facebook, and WhatsApp over 5 months for four different courses. A general-purpose lexical analyzer was designed for sentiment analysis using the SALE algorithm, which assigns a unified score between -1 and +1 to each word based on sentiment. Seed words expressing sentiments specific to teaching were defined, and their scores were compared with the provided data list. The interpretation of data reflected measures of knowledge, understanding, course contents, teaching style, and assessment. Social media data collection, keywords, and seed words related to course review and feedback were processed, and automated algorithm results using semantic analysis were presented. The survey forms designed on the same measures as social media analysis showed similar patterns. The average score in the student surveys for knowledge and understanding for all courses was 76%, while in the automated analysis, it was 71%. The results indicated that automated analysis represents a more accurate student feedback and can be effectively used to improve the curriculum and teaching methodologies. The comparison between survey forms and social media data revealed differences in scores for knowledge and understanding, teaching methodology, and exam assessments. The study suggested using online examination tools and other assessment procedures to improve the quality of assessments and enhance transparency in examinations.\n",
        "In conclusion, the results section presented the methodology and findings of the sentiment analysis, comparing automated analysis with survey forms and social media data to assess student feedback on various course aspects.\n",
        "''', '''In this evaluation section of the scientific article, the effectiveness of sentiment analysis in portfolio management is assessed by constructing sentiment-based portfolios using different language models: OPT, BERT, FinBERT, and the Loughran-McDonald dictionary model. Various investment strategies are employed, including long, short, and long-short portfolios based on positive and negative sentiment scores. Benchmark comparisons with value-weighted and equal-weighted market portfolios are included to evaluate these strategies. The long-short OPT strategy demonstrates the most robust risk-adjusted performance, as evidenced by its superior Sharpe ratio. Additionally, the performance of trading strategies based on news sentiment from August 2021 to July 2023 is examined, highlighting the impressive gains of the long-short OPT strategy and other advanced language models, in contrast to traditional market portfolios. The analysis substantiates the importance of employing sophisticated language models in developing investment strategies based on news sentiment, as they exhibit powerful predictive capability in forecasting market movements.''', '''Summary of the Evaluation Section\n",
        "The evaluation section discusses the application of a predictive model in the cyber risk insurance industry. In a scenario involving a cyber-attack on \"X\" bank using \"apache struts\" vulnerability exploits, the model effectively captures and analyzes relevant conversations to automate reports for insurance companies. These reports provide early warnings to policyholders, recommending preventive measures such as vulnerability patching and tight monitoring to minimize the potential spread of risk incident claims. The article suggests changes in the insurance company's value chain activities, including the addition of a function to manage and prevent risk occurrence. These changes aim to minimize the potential spread of risk and claims, potentially leading to new value-added services and revenue generation opportunities. The research illustrates how the predictive model can be beneficial for insurance companies and their clients in providing early warnings and additional services to mitigate cyber risk.\n",
        "''', '''The evaluation section of the article presents the results of the language models evaluated. Two approaches were taken: English- and Spanish-specific bifactor models and construct distinct language models. The first approach resulted in a four-factor model with good fit indices, indicating that combining all English measures into one construct and keeping the Spanish constructs separate was the best approach. The second approach revealed a potential problem with the lack of discrimination between combined vocabulary–grammar English and higher level language English. This led to the specification of a four-factor model as the best emerging model, supported by comprehensive analysis and previous empirical evidence of the unidimensionality of English. The four-factor model provides information about the interfactor correlation between the English and Spanish constructs and is consistent with previous empirical evidence.\n",
        "The text presents detailed statistical findings and model comparisons, emphasizing the importance of factor discrimination, model fit indices, and previous empirical evidence in determining the best language model. The results highlight the complexity of the language constructs and the need for a parsimonious yet comprehensive model to accurately represent the English and Spanish language factors.\n",
        "''', '''Summary of Results and Analysis from Scientific Article in Computer Science\n",
        "The evaluation section of the scientific article presents the results of two experiments aimed at detecting colorblindness using an interactive method. The first experiment revealed that individual features, such as agility or reflex, negatively influenced the diagnostic process, leading to a higher number of errors and classifying almost 2/3 more individuals as potentially colorblind compared to the second experiment. However, all confirmed cases of colorblindness were detected and diagnosed properly, with any \"false positives\" later excluded by expert evaluation. The second experiment showed that switching from a 2D to a 3D environment had almost no impact on the diagnostic process. The results indicated that the simplest game, \"select a square,\" was the most optimal for colorblindness detection in terms of speed, simplicity, and a relatively low random error rate, making it suitable for a wider audience. The article concludes that the proposed interactive method for detecting colorblindness is an interesting alternative to currently used methods, especially for screening tests, and emphasizes the importance of prioritizing quick diagnosis in such methods.\n",
        "The results of the experiments are presented in Tables 2 and 3, detailing the diagnosis outcomes for both the colorblind and control groups. The tables provide a comprehensive breakdown of the diagnosed persons, true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) for each type of color vision deficiency and the control group.\n",
        "Overall, the article suggests that the proposed interactive method for detecting colorblindness shows promise as an alternative to current methods, particularly for screening tests, and emphasizes the importance of prioritizing quick diagnosis. The results of the experiments and the analysis of the diagnostic process provide valuable insights into the effectiveness of different interactive methods for colorblindness detection.\n",
        "''']\n",
        "\n",
        "\n",
        "summaries_PZSH = ['''The evaluation section of the scientific article reveals that the model's performance improved with up to three levels of input sequence encoding, as evidenced by increased Rouge scores. However, performance declined with four or more encoding levels. Computational costs increased with more encoding levels, but weight-sharing helped mitigate this. The model outperformed baselines on the Gigaword Test Corpus, with significant improvements in Rouge scores at three encoding levels. Human evaluations confirmed the model's summaries were grammatically correct and readable, with informativeness and veracity also improving at higher encoding levels. Inter-rater reliability was high, indicating consistent human evaluations.''', '''In a study evaluating machine learning (ML) algorithms for predicting infection risk after surgery for tibial shaft fractures (TSFs), 1,822 patients were analyzed. The median age was 38, with 75% being men. The study identified 7 key variables for algorithm development, including fracture classification and patient age.\n",
        "The ML models' performance varied, with the best-performing models (penalized logistic regression, Bayes point machine, and boosted decision tree) showing AUCs between 0.80 and 0.82. The final model, based on penalized logistic regression, had a calibration slope of 0.94 and intercept of 0.00, with a Brier score of 0.078.\n",
        "The study also developed an online prediction tool incorporating the final model, allowing for postoperative infection risk assessment. The tool demonstrated the ability to predict infection probabilities for different patient scenarios.\n",
        "''', '''The evaluation section of the article presents the accuracy of a proposed method in determining semantic relatedness within the Solar domain. The method's accuracy was assessed by comparing the Pearson correlation between human-computed similarity scores and the similarity scores generated by the proposed method. The results yielded an accuracy of 0.863, indicating a high level of correlation with human judgment.\n",
        "The study compared its method against three standard datasets: Miller & Charles (M&C), Robenstein & Goodenough (R&G), and the 353 word similarity dataset. The comparison showed that the proposed method outperformed previously defined measures in these datasets.\n",
        "The results are summarized in Table 6, which lists various measures, including Path length, Edge-Method, Wn&Palmer, Lin, IC(Corpus), Jiang & Conrath's IC(Corpus), Tversky's Feature, Feature-based Approach using Wikipedia, and our proposed Similarity(our approach) method. The table demonstrates that the proposed method achieved the highest accuracy scores across the different measures, with the highest being 0.863 for the Similarity(our approach) method.\n",
        "''', '''This section evaluates the proposed solution's performance through experiments. The setup involved an 8-node parallel cluster with specific hardware configurations, using R and C++ for development. Two data sets from the UCI repository were used, with subsets created for analysis. Descriptive statistics and machine learning model computations were benchmarked against Spark and a parallel DBMS, Vertica. Our solution showed competitive performance with Spark, especially when data was already partitioned. Computing descriptive statistics on subsets was efficient, with minimal performance impact across different data sizes. However, for large datasets, a parallel cluster was more effective than a single machine due to overheads associated with data partitioning and transfer. The study highlights the trade-offs between single-machine and parallel processing, suggesting the former is preferable for smaller datasets.''', '''In the evaluation section of the scientific article, the authors present the outcomes of their experiments within the field of computer science. They meticulously detail the results achieved, emphasizing the significance of these findings. The experiments conducted are aimed at testing the efficacy of a particular computational method or algorithm. The results are quantified, with specific figures and statistics highlighting the performance improvements or advancements made. The authors interpret these results, explaining how they contribute to the broader understanding of the subject matter. They also discuss potential implications for future research and practical applications. The summary retains all critical data and numbers, ensuring that the essence of the original text is preserved while adhering to the length constraint.''', '''The article presents the results of experiments conducted over a semester using data from Blackboard, Facebook, and WhatsApp for four different courses. A lexical analyzer was developed for sentiment analysis, assigning scores between -1 and +1 to words based on their sentiment. The study focused on teaching-related sentiments, assigning positive, negative, and neutral scores to each word. The analysis revealed four key areas: knowledge and understanding, course contents, teaching style, and assessment.\n",
        "The survey results showed an average score of 76% for knowledge and understanding across all courses, while the automated analysis yielded a slightly lower score of 71%. The teaching method received high scores in surveys (>80%), but the automated analysis showed less than 70%. The discrepancy may be due to students feeling more comfortable expressing their opinions on social media.\n",
        "For the Computer Architecture course, survey results showed an 80% satisfaction rate for knowledge and understanding, while social media analysis showed a 70% rate. This difference could be attributed to the ongoing discussion about the course throughout the semester.\n",
        "Regarding assessments, social media data showed an average score of 65%, while survey results indicated a higher average of 70%. Students expressed dissatisfaction with exam results on social media, which may have influenced the lower scores in automated analysis.\n",
        "Overall, the study suggests that automated sentiment analysis can provide valuable insights into student feedback, complementing traditional survey methods. The findings can help improve curriculum and teaching methodologies.\n",
        "''', '''3. Results 3.1. Sentiment Analysis Accuracy in U.S. Financial News In this study, we used LLMs to analyze sentiment in U.S. financial news. We processed a dataset of 965,375 articles from Refinitiv, spanning from January 1, 2010, to June 30, 2023. We used 20% of these articles as a test set. We measured the accuracy of each model in predicting the direction of stock returns based on news sentiment. This accuracy indicates how well the model links the sentiment in financial news with stock returns over a three-day period. We evaluated four models: OPT, BERT, FinBERT and the Loughran-McDonald dictionary. Their performance in sentiment analysis is shown in Table 3. Table 3 Language model performance metrics: accuracy, precision, recall, specificity, and the F1 score for each model. The OPT model is the most accurate, followed closely by BERT and FinBERT. Metric OPT BERT FinBERT Loughran-McDonald Accuracy 0.744 0.725 0.722 0.501 Precision 0.732 0.711 0.708 0.505 Recall 0.781 0.761 0.755 0.513 Specificity 0.711 0.693 0.685 0.522 F1 score 0.754 0.734 0.731 0.508 The results show that the OPT model is the most accurate, followed closely by BERT and FinBERT. The Loughran-McDonald dictionary, a traditional finance text analysis tool, has significantly lower accuracy. This indicates that language models like OPT, BERT, and FinBERT are better at understanding and analysing complex financial news. The precision and recall values further support the superiority of the OPT model; its F1 score, which combines precision and recall, also confirms its effectiveness in sentiment analysis. These findings confirm that language models, particularly OPT, are valuable tools''', '''The evaluation section of the article presents the analysis of data collected from Twitter and the CVE database. The data was cleaned, organized, and analyzed using various methods such as histogram analysis, word cloud and commonality analysis, cluster dendrogram analysis, and pyramid analysis.\n",
        "The histogram analysis revealed that the cyber risk was occurring through Apache, Apache Struts, Yahoo, and Cisco, with the attack methods using the interactive shell, strutsshell interactive, kitploitstrutsshell, and strutspwn exploit. The word cloud and commonality analysis validated that the keywords used in the Twitter and CVE data collection process were appropriate, as there were discussions about vulnerabilities in both platforms.\n",
        "The cluster dendrogram analysis showed that the terms Apache, Struts, Shell Command, Interactive, and Exploit were found in adjacent clusters, validating that the cyber risk attack on Apache Struts was using the interactive shell command exploits. The pyramid analysis showed that Apache was the third most frequent unigram in Twitter, compared to the CVE database, validating that Apache had the highest cyber risk frequency.\n",
        "The data was then used to train and test a prediction model using SML. The model was able to predict the occurrence of cyber risk events with an accuracy rate of 96.73%. The model was also able to identify the most frequently discussed topics related to cyber risk, such as Apache, Struts, and Vulnerability.\n",
        "Overall, the evaluation section of the article demonstrates the effectiveness of using data analysis methods to identify and predict cyber risk events. The results of this research can be useful for insurance companies to provide early warning to their clients and prevent the spread of risk occurrence claims.\n",
        "''', '''The study aimed to evaluate language measures using various models. Initially, descriptive statistics and correlations were examined, revealing no severe departures from normality and significant correlations among measures. Two approaches were considered: a bifactor model with a common language factor and language-specific factors for English and Spanish, and construct-specific models based on LARRC. The bifactor model showed the best fit, with acceptable and robust standard errors. However, the construct-specific models also provided valuable insights, particularly regarding the intercorrelations among vocabulary, grammar, and higher-level language constructs. The four-factor model from the second approach, combining English vocabulary and grammar with higher-level language, was chosen due to its good fit and alignment with previous research. This model also highlighted a high correlation between Spanish grammar and higher-level language constructs, suggesting potential for combining these factors.''', '''Experiment 1 used a game where participants had to select a square, while Experiment 2 introduced a 3D environment with the Colorblind Maze game. Results were recorded in Tables 2 and 3.\n",
        "Table 2 showed that in Experiment 1, all confirmed colorblind cases were accurately diagnosed. However, individual features like agility or reflex negatively impacted the diagnostic process, leading to a higher number of false positives.\n",
        "In Experiment 2, the 3D environment had minimal effect on the diagnostic process, with results similar to the 2D game. However, the development and computing power required for the 3D game made it less practical for screening tests.\n",
        "The analysis concluded that the simplest game, \"select a square,\" was the most optimal for colorblindness detection due to its speed, simplicity, and low error rate. Despite the potential of all proposed games, the technical requirements and time needed for the 3D game made it less favorable for widespread use.\n",
        "''' ]\n",
        "\n",
        "\n",
        "summaries_PFSH = ['''The study evaluates sentiment analysis in U.S. financial news using language models (LLMs) on a dataset of 965,375 articles. Four models were assessed: OPT, BERT, FinBERT, and Loughran-McDonald. OPT showed the highest accuracy, with regression analyses indicating a strong correlation with next-day stock returns. Advanced LLMs like OPT, FinBERT, and BERT outperformed traditional models and were superior for portfolio management strategies. The study concluded that sophisticated LLMs are crucial for accurate sentiment analysis and investment decision-making in financial markets.\n",
        "Additionally, the study examined the performance of models with varying levels of input sequence encoding. It found that performance improved with up to three encoding levels but declined with higher levels. Computational costs increased with encoding levels, but weight-sharing across levels mitigated this. The study also reported on the models' performance on the Gigaword and DUC2004 corpora, with MED models outperforming baselines. Human evaluations confirmed the high quality of summaries generated by models with three levels of encoding. The study emphasized the importance of encoding levels and computational considerations in model performance and practical applications.\n",
        "''', '''A machine learning algorithm was developed to predict the risk of infection after surgery for tibial shaft fractures. The algorithm was based on a model that outperformed others in terms of calibration and predictive performance. The final model, derived from a penalized logistic regression algorithm, was chosen for its superior calibration and similar performance to other models. The model incorporated several patient and fracture characteristics, including the Gustilo-Anderson classification, bone loss, mechanism of injury, multitrauma, AO/OTA classification, age, and fracture location. These factors were identified as the strongest predictors of infection. The algorithm's performance was validated in a separate cohort, maintaining high accuracy. An online prediction tool was created to facilitate the calculation of infection risk for patients undergoing operative treatment for tibial shaft fractures.''', '''The study evaluates semantic relatedness in the Solar domain using global ontologies and various similarity measures. It compares human-computed similarities with those proposed by the method, achieving an accuracy of 0.863. The method's performance is assessed against standard datasets and measures, outperforming most of them.''', '''This section details the experimental evaluation of the proposed solution, including the setup, data, and comparison with other systems. Experiments were conducted on an 8-node cluster with specifications and using R and C++. Data sets YearPredictionMSD and CreditCard were used, replicated to simulate larger data volumes. Descriptive statistics were computed on subsets of data, and machine learning models were compared with Spark and a parallel DBMS, Vertica. The study also explores the trade-offs between single-machine and parallel processing, showing that smaller data sets can be efficiently handled by a single machine, while larger data sets benefit from parallel processing despite initial partitioning overhead.''', '''The summary provided is a comprehensive overview of the findings from the evaluation section of a study on sentiment analysis in U.S. financial news using language models (LLMs). The study processed 965,375 articles and evaluated four models: OPT, BERT, FinBERT, and Loughran-McDonald. OPT demonstrated the highest accuracy, followed by BERT and FinBERT, while the traditional Loughran-McDonald model lagged significantly behind. Regression analyses revealed OPT's strong correlation with next-day stock returns, followed by FinBERT and BERT. Factors like model design and training data specificity contribute to these differences. Portfolio management strategies based on sentiment analysis further demonstrated the superior performance of advanced LLMs compared to traditional methods. Long-short OPT strategy outperformed others significantly, indicating the predictive capability of advanced LLMs in forecasting market movements. Overall, the study emphasizes the importance of employing sophisticated language models for accurate sentiment analysis and investment decision-making in financial markets.\n",
        "The summary also discusses the trends in research in various applications year-wise, highlighting the increase in research in MSR from 2010 to April 2018. It notes that approximately 48 percent of the papers belong to summarization and sentiment analysis, making these topics the most popular in the NLP field. The summary also touches on the increase in research works in mobile analytics from 2012 to 2015, the saturation stage of research in the field of context-based sentiment analysis, and the challenges in norms mining in open-source development communities.\n",
        "Furthermore, the summary addresses the future scope and challenges in the field of sentiment analysis, norms mining, mobile analytics, and summarization techniques. It highlights the need for more research in context-based sentiment analysis, norms mining in open-source development communities, and the summarization of software artifacts. The summary also discusses the challenges in handling massive mobile user review data, the need for better ways to evaluate summaries, and the challenges in creating abstractive summaries.\n",
        "Lastly, the summary mentions the need for more research on handling heterogeneous data consisting of code fragments from\n",
        "''', '''The study analyzed sentiment in educational social media data across four parameters: knowledge and understanding, course content, teaching style, and assessment. Data from Blackboard, Facebook, and WhatsApp over six months were processed using a lexical analyzer and sentiment scoring algorithm. Survey results, collected at the semester's end, were compared with automated analysis. The study found survey results for knowledge and understanding to be higher than automated analysis, with a notable discrepancy in teaching style ratings. Social media sentiment analysis revealed ongoing discussions and dissatisfaction with exam results. The study suggests that automated sentiment analysis can provide more accurate student feedback for curriculum improvement.''', '''The study examined the accuracy of sentiment analysis in U.S. financial news using language models (LLMs). It processed 965,375 articles and evaluated four models: OPT, BERT, FinBERT, and the Loughran-McDonald dictionary. The OPT model demonstrated the highest accuracy, followed by BERT and FinBERT. The traditional Loughran-McDonald model lagged significantly behind. Regression analyses showed OPT's strong correlation with next-day stock returns, followed by FinBERT and BERT. Factors such as model design and training data specificity contribute to these differences. Portfolio management strategies based on sentiment analysis further demonstrated the superior performance of advanced LLMs compared to traditional methods. The long-short OPT strategy outperformed others significantly, indicating the predictive capability of advanced LLMs in forecasting market movements. Overall, the study emphasizes the importance of employing sophisticated language models for accurate sentiment analysis and investment decision-making in financial markets.\n",
        "The study also evaluated the predictive power of various LLMs in forecasting stock returns. The regression results showed that the OPT model had the strongest correlation with next-day stock returns, followed by BERT and FinBERT. The Loughran-McDonald dictionary model had the least predictive power. The study also found that the predictive strength of the models increased when both LLMs were used as independent variables in the same regression.\n",
        "The study further assessed the performance of sentiment-based portfolios. It constructed various sentiment-based portfolios using sentiment scores derived from different language models. The long-short OPT strategy demonstrated the most robust risk-adjusted performance, as evidenced by its superior Sharpe ratio. The Loughran-McDonald dictionary model-based strategy lagged behind, particularly when compared to the value-weighted market portfolio.\n",
        "The study concluded that advanced language models, particularly OPT, are valuable tools for analyzing financial news and predicting stock market trends. The study also highlighted the importance of model selection in sentiment-based trading and the superiority of advanced LLMs in guiding investment decisions.\n",
        "''', '''The study analyzed sentiment in U.S. financial news using language models (LLMs). It processed 965,375 articles and evaluated four models: OPT, BERT, FinBERT, and Loughran-McDonald. OPT demonstrated the highest accuracy, followed by BERT and FinBERT. The traditional Loughran-McDonald model lagged significantly behind. The study also performed regression analyses, which revealed OPT's strong correlation with next-day stock returns. The study concluded that advanced LLMs outperformed traditional methods in portfolio management strategies based on sentiment analysis. The study emphasized the importance of using sophisticated language models for accurate sentiment analysis and investment decision-making in financial markets.''', '''The study utilized descriptive statistics and correlation analyses on a range of language measures from various tests to assess their relationship. The data showed no severe abnormalities, and analyses were adjusted for slight nonnormality. Two approaches were taken to model language data: a bifactor model considering both English and Spanish factors, and a construct-specific model based on previous research. The bifactor model had acceptable fit indices, but faced issues with the orthogonality constraint. The construct-specific model had varying fit across models, with the four-factor model being the most supported due to better discrimination and alignment with previous findings of English language unidimensionality in kindergarten. The study suggests the four-factor model as the best representation of language constructs, considering complexity and parsimony.''', '''The study conducted by Maciej Laskowski explored the effectiveness of interactive methods for detecting colorblindness. The analysis of experiments using the proposed method revealed it as a promising alternative to existing methods. The first experiment indicated that certain individual features, such as agility or reflex, negatively impacted the diagnostic process, leading to a higher number of false positives. However, all confirmed cases of colorblindness were accurately diagnosed. The transition from a 2D to a 3D environment had minimal effect on the diagnostic process. Despite the potential of the proposed games for colorblindness detection, the simplest game, \"select a square\", proved to be the most optimal in terms of speed, simplicity, and low error rate. Its low technical requirements and straightforward rules make it accessible to a broader audience.''']"
      ],
      "metadata": {
        "id": "FdIrwu3qzt5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from bert_score import BERTScorer\n",
        "\n",
        "import csv\n",
        "\n",
        "fields = ['ChatGPT','Google BARD/Gemini','YouChat', 'Phi3 ZeroShot', 'Phi3 FewShot']\n",
        "\n",
        "text = []\n",
        "\n",
        "for i in range(len(evaluations)):\n",
        "  reference = evaluations[i]\n",
        "  candidate_CGPT = summaries_CGPT[i]\n",
        "  candidate_BARD = summaries_BARD[i]\n",
        "  candidate_YCHT = summaries_YCHT[i]\n",
        "  candidate_PZSH = summaries_PZSH[i]\n",
        "  candidate_PFSH = summaries_PFSH[i]\n",
        "\n",
        "  scorer = BERTScorer(model_type='bert-base-uncased')\n",
        "  P_CGPT, R_CGPT, F1_CGPT = scorer.score([candidate_CGPT], [reference])\n",
        "\n",
        "  P_BARD, R_BARD, F1_BARD = scorer.score([candidate_BARD], [reference])\n",
        "\n",
        "  P_YCHT, R_YCHT, F1_YCHT = scorer.score([candidate_YCHT], [reference])\n",
        "\n",
        "  P_PZSH, R_PZSH, F1_PZSH = scorer.score([candidate_PZSH], [reference])\n",
        "\n",
        "  P_PFSH, R_PFSH, F1_PFSH = scorer.score([candidate_PFSH], [reference])\n",
        "\n",
        "  P_score = [P_CGPT, P_BARD, P_YCHT, P_PZSH, P_PFSH]\n",
        "  R_score = [R_CGPT, R_BARD, R_YCHT, R_PZSH, R_PFSH]\n",
        "  F1_score = [F1_CGPT, F1_BARD, F1_YCHT, F1_PZSH, F1_PFSH]\n",
        "\n",
        "  text.append(P_score)\n",
        "  text.append(R_score)\n",
        "  text.append(F1_score)\n",
        "  text.append(['---', '---', '---', '---', '---'])\n",
        "\n",
        "filename = \"BERT_Scores.csv\"\n",
        "\n",
        "with open(filename, 'w') as csvfile:\n",
        "    # creating a csv writer object\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "\n",
        "    # writing the fields\n",
        "    csvwriter.writerow(fields)\n",
        "\n",
        "    # writing the data rows\n",
        "    csvwriter.writerows(text)"
      ],
      "metadata": {
        "id": "lp2YUZJcw7cb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}